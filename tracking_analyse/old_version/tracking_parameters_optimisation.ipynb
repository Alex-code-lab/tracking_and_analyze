{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Aug 21 10:51:23 2023\n",
    "\n",
    "@author: souchaud\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import imageio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import trackpy as tp\n",
    "from tqdm import tqdm\n",
    "# import functions_track_and_analyze as lib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import cv2\n",
    "from skimage import util\n",
    "# from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidated parameters\n",
    "PARAMS = {\n",
    "        # Préparation images\n",
    "        'GaussianBlur': (31, 31), # (19, 19), #  (5, 5),\n",
    "        'sigmaX': 10,\n",
    "        'sigmaY': 10, \n",
    "        'threshold': 5, # 10, #12, # 3, # 10  # 40,\n",
    "        'percentile': 10, #10,\n",
    "        'lenght_study': 50, # Découpage de la manip en nombre de frale pour favoriser l'étude (performence ordi)\n",
    "        'smoothing_size': None,\n",
    "        'invert': False, # True,\n",
    "        'preprocess': True, \n",
    "        'characterize': True,\n",
    "        'filter_before': None,\n",
    "        'filter_after': None,\n",
    "        # Paramètres Manip\n",
    "        'pixel_size': 0.637,  # 1.2773, # en um\n",
    "        'frame_interval': 15, # temps entre chaque frame\n",
    "        'long_time': False,\n",
    "        'max_frame': 340, # 340, #340 # Nombre de frame d'étude max.\n",
    "        'min_frames': 150, #150, # Nombre de frame sur laquelle doit être suivie une cellule\n",
    "        'topn': 120, # None, # Nombre de particules max à détecter\n",
    "\n",
    "        # Détéction particules\n",
    "        'diameter': 55, #51,  # 15, # Diamètres évalué des particules\n",
    "        'max_displacement': 30, # 35, # 25, # Déplacement maximal des cellules entre deux images (en pixel)\n",
    "        'search_range': 30, #  30, #  20 # même chose\n",
    "        'minmass': 500, #  Mass minimale mesurée des cellules\n",
    "        'max_size': 40, # 25, # Taille maximum de la particule\n",
    "        'separation': 50, # 9, # distance mimimanl pour séparé deux objets\n",
    "        'noise_size': 3,  # 7, # 7, # taille des particules à exclure \n",
    "        'max_iterations': 15, # Nombre d'itérations max pour résoudre un sous-réseau (déterminer les trajectoires entre 2 cellules)\n",
    "        'memory': 5, # Nombre de frame au dela de laquelle on oublie la cellule\n",
    "        'engine': 'auto',\n",
    "\n",
    "        # Format et chemins\n",
    "        'remove_exts': ['.jpg', '.svg', 'hdf5', '.png'],   \n",
    "        'data_dir': '/Users/souchaud/Desktop/A_Analyser/CytoOne_HL5_10x/',\n",
    "        # 'data_dir': '/Volumes/Labo_Alex_Mac/A_analyser/CytoOne_HL5/',\n",
    "        # 'data_dir': '/Users/souchaud/Desktop/A_Analyser/NonT_SorC/',\n",
    "        # 'output_dir': '/Users/souchaud/Desktop/Analyses/CytoOne_HL5_longtime/'\n",
    "        # 'data_dir': '/Volumes/Labo_Alex_Mac/A_analyser/CytoOne_HL5/',´\n",
    "        'output_dir': '/Users/souchaud/Desktop/Analyses/CytoOne_HL5_10x_new_param/'\n",
    "        # 'output_dir': '/Users/souchaud/Desktop/Analyses/NonT_SorC_longtime_New/'\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAMES = [f + '/mosaic/' for f in os.listdir(PARAMS['data_dir'])\n",
    "                    if os.path.isdir(os.path.join(PARAMS['data_dir'], f))]\n",
    "print(EXPERIMENT_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_speed(filtered):\n",
    "    \"\"\"\n",
    "    Compute mean speed.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    - filtered: DataFrame with tracked cells\n",
    "    Returns\n",
    "    - mean_speed: Mean speed of all particles\n",
    "    - mean_speed_part: Mean speed per particle\n",
    "    \"\"\"\n",
    "    dx = filtered.groupby('particle')['x'].diff()\n",
    "    dy = filtered.groupby('particle')['y'].diff()\n",
    "    displacement = np.sqrt(dx**2 + dy**2)\n",
    "    duration = filtered.groupby('particle')['frame'].diff() * PARAMS['frame_interval']\n",
    "    mean_speed = (displacement.sum() / duration.sum()) * PARAMS['pixel_size'] * 60\n",
    "    instant_speed = displacement / duration\n",
    "    mean_speed_part = instant_speed.groupby(filtered['particle']).mean() * PARAMS['pixel_size'] * 60\n",
    "    return mean_speed, mean_speed_part\n",
    "\n",
    "\n",
    "def clean_directory(dir_path):\n",
    "    \"\"\"Remove all files with the specified extensions in the directory.\"\"\"\n",
    "    for file in os.listdir(dir_path):\n",
    "        if file.endswith(tuple(PARAMS['remove_exts'])):\n",
    "            os.remove(os.path.join(dir_path, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_brightness_contrast(img, brightness=0, contrast=0):\n",
    "    \"\"\" Ajuster la luminosité et le contraste d'une image \"\"\"\n",
    "    B = brightness / 100.0\n",
    "    C = contrast / 100.0\n",
    "    k = np.tan((45 + 44 * C) / 180 * np.pi)\n",
    "\n",
    "    img = (img - 127.5 * (1 - B)) * k + 127.5 * (1 + B)\n",
    "    img = np.clip(img, 0, 255).astype(np.uint8)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_experiment(exp_name, PARAMS):\n",
    "    \"\"\"Process a single experiment.\"\"\"\n",
    "    output_path = os.path.join(PARAMS['output_dir'], exp_name)\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    clean_directory(output_path)\n",
    "\n",
    "    experiment_data_dir = os.path.join(PARAMS['data_dir'], exp_name)\n",
    "\n",
    "    def extract_number(filename):\n",
    "        # Extrait le numéro à partir du nom de fichier\n",
    "        base_name = os.path.basename(filename)\n",
    "        # Supprime l'extension et extrait le numéro\n",
    "        number = int(base_name.split('_')[-1].split('.')[0])\n",
    "        return number\n",
    "\n",
    "    tiff_files = sorted(glob.glob(os.path.join(experiment_data_dir, \"*.tif\")), key=extract_number)\n",
    "\n",
    "    # Use PARAMS dictionary to get the parameters\n",
    "    frame_data = []\n",
    "    frame_counter = 0\n",
    "    boucle = []\n",
    "    if PARAMS['long_time'] is False:\n",
    "        if len(os.listdir(experiment_data_dir)) < PARAMS['max_frame']:\n",
    "            nbr_frame_study_total = len(os.listdir(experiment_data_dir))\n",
    "        else:\n",
    "            nbr_frame_study_total = PARAMS['max_frame']\n",
    "    else:\n",
    "        nbr_frame_study_total = len(os.listdir(experiment_data_dir))\n",
    "\n",
    "    lenght_study = PARAMS['lenght_study']\n",
    "    if nbr_frame_study_total > lenght_study:\n",
    "        number = lenght_study\n",
    "        while number < nbr_frame_study_total:\n",
    "            boucle.append(lenght_study)\n",
    "            number += lenght_study\n",
    "            if number > nbr_frame_study_total:\n",
    "                boucle.append(nbr_frame_study_total - len(boucle) * lenght_study)\n",
    "        nbr_frame_study = lenght_study\n",
    "    else:\n",
    "        nbr_frame_study = nbr_frame_study_total\n",
    "        boucle.append(nbr_frame_study)\n",
    "\n",
    "    # Process each batch of frames\n",
    "    import time\n",
    "    for i in tqdm(boucle, desc=\"processing batches\"):\n",
    "        batch_frames = tiff_files[frame_counter:frame_counter + i]\n",
    "        batch_data = [np.array(imageio.imread(tiff_file)) for tiff_file in batch_frames]\n",
    "        time_count = time.time()\n",
    "\n",
    "        # luminosity = []\n",
    "        # for num, frame in enumerate(batch_data):\n",
    "        #     luminosity.append(np.mean(frame))\n",
    "        # # Calculer la luminosité moyenne sur l'ensemble du film\n",
    "        # average_luminosity = np.mean(luminosity)\n",
    "        from scipy import ndimage\n",
    "        for num, frame in enumerate(batch_data):\n",
    "\n",
    "            # # Calculer le seuil correspondant au percentile donné\n",
    "            # # np.percentile retourne la valeur sous laquelle un certain pourcentage des valeurs tombe\n",
    "            # threshold = np.percentile(frame, 100 - 0.1)\n",
    "            # # Créer un masque binaire : 1 pour les pixels >= seuil, 0 sinon\n",
    "            # mask = frame >= threshold\n",
    "            # # Appliquer le masque sur l'image : conserver les pixels originaux où le masque est vrai\n",
    "            # result = np.zeros_like(frame)\n",
    "            # result[mask] = frame[mask]\n",
    "            # # Appliquer un filtre Gaussien\n",
    "            # # _, frame = cv2.thresold(frame, thresh=75, maxval=255, type=cv2.THRESH_BINARY\n",
    "            # frame = cv2.GaussianBlur(result, ksize=PARAMS['GaussianBlur'], sigmaX=10, sigmaY=10, )  # Modifier (5, 5) et 0 selon les besoins\n",
    "            # frame = util.invert(frame)\n",
    "            # # Create a CLAHE object (Arguments are optional)\n",
    "            # clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(10, 10))\n",
    "            # # Apply CLAHE\n",
    "            # frame = clahe.apply(frame)\n",
    "            \n",
    "            # Prétraitement\n",
    "            blurred = ndimage.median_filter(frame, size=8)\n",
    "            blurred = cv2.GaussianBlur(blurred, (5, 5), 0)\n",
    "            # equalized = cv2.equalizeHist(blurred)\n",
    "            frame = blurred\n",
    "            # frame = util.invert(blurred)\n",
    "\n",
    "            # image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "            # blurred = cv2.GaussianBlur(image, (3, 3), 0)\n",
    "\n",
    "           \n",
    "\n",
    "            # from scipy.ndimage import grey_opening\n",
    "            # ball_radius = 5\n",
    "            # # Use grey opening to simulate the rolling ball algorithm\n",
    "            # structure_element = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (2*ball_radius + 1, 2*ball_radius + 1))\n",
    "            # background = grey_opening(frame, structure=structure_element)\n",
    "            # # Subtract the background\n",
    "            # frame = cv2.subtract(frame, background)\n",
    "\n",
    "\n",
    "            # Appliquer un filtre Gaussien\n",
    "            # frame = cv2.GaussianBlur(frame, (5, 5), 0)  # Modifier (5, 5) et 0 selon les besoins\n",
    "\n",
    "            # current_luminosity = np.mean(frame)\n",
    "            # # Calculer l'ajustement nécessaire\n",
    "            # brightness_adjustment = (average_luminosity - current_luminosity) * 100 / 255\n",
    "            # frame = adjust_brightness_contrast(frame, brightness=brightness_adjustment)\n",
    "\n",
    "            batch_data[num] = frame\n",
    "\n",
    "            if num == 0:\n",
    "                plt.figure(figsize=(12, 6))\n",
    "                plt.imshow(frame, cmap='gray')\n",
    "                plt.show()\n",
    "            \n",
    "                f = tp.locate(frame,\n",
    "                              diameter=PARAMS['diameter'],\n",
    "                              minmass=PARAMS['minmass'],\n",
    "                              maxsize=PARAMS['max_size'],\n",
    "                              separation=PARAMS['separation'],\n",
    "                              noise_size=PARAMS['noise_size'],\n",
    "                              smoothing_size=PARAMS['smoothing_size'],\n",
    "                              threshold=PARAMS['threshold'],\n",
    "                              invert=PARAMS['invert'],\n",
    "                              percentile=PARAMS['percentile'],\n",
    "                              topn=PARAMS['topn'],\n",
    "                              preprocess=PARAMS['preprocess'],\n",
    "                              max_iterations=PARAMS['max_iterations'],\n",
    "                              filter_before=PARAMS['filter_before'],\n",
    "                              filter_after=PARAMS['filter_after'],\n",
    "                              characterize=PARAMS['characterize'],\n",
    "                              engine=PARAMS['engine'])\n",
    "                print(len(f))\n",
    "\n",
    "                transformed_image_path = os.path.join(output_path, f'transformed_frame_{i}.jpg')\n",
    "                cv2.imwrite(transformed_image_path, frame)\n",
    "                #             diameter=7,\n",
    "                #               minmass=50, maxsize=15,\n",
    "                #               separation=10, invert=PARAMS['invert'],\n",
    "                #               characterize=True, engine='auto')\n",
    "                fig, ax = plt.subplots(figsize=(12, 6))\n",
    "                tp.annotate(f, frame, ax=ax)\n",
    "                display(fig)\n",
    "\n",
    "                # Extract size information\n",
    "                # The specific column name depends on your data; it might be 'size', 'mass', etc.\n",
    "                sizes = f['size']\n",
    "\n",
    "                # Plot the particle sizes\n",
    "                # plt.figure(figsize=(12, 6))\n",
    "                fig, ax = plt.subplots(figsize=(12, 6))\n",
    "                plt.hist(sizes, bins=30)\n",
    "                plt.xlabel('Particle size')\n",
    "                plt.ylabel('Frequency')\n",
    "                plt.title('Particle Size Distribution')\n",
    "                plt.savefig(os.path.join(output_path, f'Hist_size_{i}.jpg'), format='jpg')  # Enregistrer avant de montrer\n",
    "                # plt.show()\n",
    "                display(fig)\n",
    "\n",
    "                # plt.figure(figsize=(12,6))\n",
    "                fig, ax = plt.subplots(figsize=(12, 6))\n",
    "                tp.mass_size(f, ax=ax)  # Assurez-vous que 'f' est correctement configuré\n",
    "                plt.savefig(os.path.join(output_path, f'mass_size_{i}.jpg'), format='jpg')\n",
    "                # plt.show()\n",
    "                # plt.close()  # Ferme la figure\n",
    "                display(fig)\n",
    "\n",
    "\n",
    "\n",
    "        print(\"temps de travail sur les images : \", (time.time() - time_count)/60, \"min\")\n",
    "        # frame[frame > 90] = 0\n",
    "        # plt.figure(figsize=(12, 6))\n",
    "        # plt.imshow(cv2.GaussianBlur(frame, (251, 251), 0), cmap='gray')\n",
    "        # plt.show()\n",
    "        # if nbr_frame_study_total <= 260:\n",
    "        try:\n",
    "            cells_loc = tp.batch(batch_data,\n",
    "                                diameter= PARAMS['diameter'], #PARAMS['diameter'],\n",
    "                                minmass=PARAMS['minmass']                                                                                                                                                                                                                                                                                                                                                                                                                                                 ,\n",
    "                                maxsize=PARAMS['max_size'],\n",
    "                                separation=PARAMS['separation'],\n",
    "                                noise_size=PARAMS['noise_size'],\n",
    "                                smoothing_size=PARAMS['smoothing_size'],\n",
    "                                threshold=PARAMS['threshold'],\n",
    "                                invert=PARAMS['invert'],\n",
    "                                percentile=PARAMS['percentile'],\n",
    "                                topn=PARAMS['topn'],\n",
    "                                preprocess=PARAMS['preprocess'],\n",
    "                                max_iterations=PARAMS['max_iterations'],\n",
    "                                filter_before=PARAMS['filter_before'],\n",
    "                                filter_after=PARAMS['filter_after'],\n",
    "                                characterize=PARAMS['characterize'],\n",
    "                                engine=PARAMS['engine'],\n",
    "                                )\n",
    "            display(cells_loc)\n",
    "            cells_loc['frame'] += frame_counter\n",
    "            frame_counter += i\n",
    "            frame_data.append(cells_loc)\n",
    "        except Exception as e:\n",
    "            print(f\"{exp_name} got an issue.\")\n",
    "            return\n",
    "\n",
    "    all_features = pd.concat(frame_data)\n",
    "\n",
    "    try:\n",
    "        trajectories = tp.link_df(all_features,\n",
    "                                  search_range=PARAMS['search_range'],  # PARAMS['max_displacement'],\n",
    "                                  memory=PARAMS['memory'],\n",
    "                                  neighbor_strategy='KDTree',\n",
    "                                  link_strategy='auto',  # 'hybrid',\n",
    "                                  adaptive_stop=30,\n",
    "                                  # verify_integritxy=True,\n",
    "                                  )\n",
    "        trajectories.to_hdf(os.path.join(output_path, 'filtered.hdf5'), 'table')\n",
    "        # verify_intetegrity=True)\n",
    "        # neighbor_strategy='KDTree',\n",
    "    except tp.SubnetOversizeException:\n",
    "        print(\"Issue with this one\")\n",
    "\n",
    "    filtered = tp.filter_stubs(trajectories, PARAMS['min_frames'])\n",
    "    # filtered = filtered[~filtered.particle.isin(\n",
    "    #     tp.filter_clusters(filtered, quantile=0.1,\n",
    "    #                        threshold=filtered['size'].mean() * 1).index)]\n",
    "    all_features.to_hdf(os.path.join(output_path, 'features.hdf5'), 'table')\n",
    "    filtered.to_hdf(os.path.join(output_path, 'filtered.hdf5'), 'table')\n",
    "\n",
    "    fig, axis = plt.subplots(figsize=(10, 10))\n",
    "    plt.title(f'Trajectories after suspicious particles {exp_name}')\n",
    "    tp.plot_traj(filtered, superimpose=batch_data[0], label=(False))\n",
    "    plt.show()\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Process all experiments.\"\"\"\n",
    "    for exp_name in EXPERIMENT_NAMES:\n",
    "        print(exp_name)\n",
    "        process_experiment(exp_name, PARAMS)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
