{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!DOCTYPE html>\n",
    "<html lang=\"fr\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <title>Analyse de Trajectoires de Tracking</title>\n",
    "    <style>\n",
    "        body {\n",
    "            font-family: Arial, sans-serif;\n",
    "        }\n",
    "        h1 {\n",
    "            color: skyblue;\n",
    "            font-size: 24px;\n",
    "        }\n",
    "        p, li {\n",
    "            font-size: 16px;\n",
    "        }\n",
    "        .green-text{\n",
    "            color: DarkSeaGreen;\n",
    "        }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>Analyse de Trajectoires de Tracking</h1>\n",
    "    <p>Ce code d'analyse permet de traiter les données issues du tracking pour extraire et étudier statistiquement les trajectoires pertinentes. Le processus est structuré comme suit :</p>\n",
    "    <ol>\n",
    "        <li><strong class=\"green-text\">Récupération des trajectoires :</strong> Collecte de toutes les trajectoires issues des différentes manipulations.</li>\n",
    "        <li><strong class=\"green-text\">Pré-analyse :</strong> Examen initial des trajectoires pour déterminer celles à conserver pour l'analyse approfondie.</li>\n",
    "        <li><strong class=\"green-text\">Analyse statistique :</strong> Application de méthodes statistiques aux trajectoires conservées. Les données peuvent être séparées en deux populations pour un traitement spécifique si nécessaire.</li>\n",
    "    </ol>\n",
    "    <p>L'analyse se concentre sur les paramètres suivants :</p>\n",
    "    <ul>\n",
    "        <li>Angle entre les directions successives d'une particule entre chaque intervalle de temps.</li>\n",
    "        <li>Vitesses moyennes et instantanées des particules.</li>\n",
    "        <li>Variation de la vitesse en fonction du temps d'incubation.</li>\n",
    "        <li>Effet potentiel de la densité de cellules sur la motilité.</li>\n",
    "    </ul>\n",
    "</body>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "<center><span style=\"color: seagreen; font-size: 50px; font-style: bold\">Chargement et préparation des données.</span></center>\n",
    "<span style=\"color: skyblue; font-size: 20px; font-style: bold\">Chargement des librairies.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Mar  1 12:46:56 2023.\n",
    "\n",
    "@author: souchaud\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import glob\n",
    "import cv2\n",
    "import warnings\n",
    "import importlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import trackpy as tp\n",
    "import seaborn as sns\n",
    "import imageio.v2 as imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import functions_analyze as lib\n",
    "from typing import List, Optional, Union, Any, Dict, Tuple\n",
    "from matplotlib import colormaps\n",
    "from cycler import cycler\n",
    "from colorama import init\n",
    "from scipy.stats import linregress\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "# Reload custom library\n",
    "importlib.reload(lib)\n",
    "\n",
    "# Initialize colorama\n",
    "init(autoreset=True)\n",
    "\n",
    "# Suppress specific warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Set default matplotlib style\n",
    "plt.style.use('default')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: skyblue; font-size: 20px; font-style: bold\">Paramètres de graphs.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.rcParams.update({\n",
    "#     # Figure\n",
    "#     \"figure.figsize\": (10, 6),  # Taille par défaut de la figure (largeur, hauteur en pouces)\n",
    "#     \"figure.dpi\": 100,  # Résolution en points par pouce\n",
    "#     \"figure.facecolor\": (0, 0, 0, 1),  # Fond de la figure : noir pur\n",
    "#     \"figure.edgecolor\": \"white\",  # Bordure de la figure en blanc\n",
    "#     \"figure.titlesize\": 20,  # Taille de la police pour le titre principal\n",
    "#     \"figure.titleweight\": \"bold\",  # Style de la police pour le titre principal : gras\n",
    "\n",
    "#     # Axes\n",
    "#     \"axes.facecolor\": (0, 0, 0, 1),  # Fond des axes : noir pur\n",
    "#     \"axes.edgecolor\": \"white\",  # Bordure des axes en blanc\n",
    "#     \"axes.linewidth\": 2,  # Épaisseur des bordures des axes\n",
    "#     \"axes.titlesize\": 16,  # Taille de la police des titres des axes\n",
    "#     \"axes.titleweight\": \"bold\",  # Style de la police pour les titres des axes : gras\n",
    "#     \"axes.labelsize\": 14,  # Taille de la police des étiquettes des axes\n",
    "#     \"axes.labelweight\": \"medium\",  # Style de la police des étiquettes : intermédiaire\n",
    "#     \"axes.labelcolor\": \"white\",  # Couleur des étiquettes des axes\n",
    "#     \"axes.prop_cycle\": cycler(color=[\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\", \"#d62728\"]),  # Cycle des couleurs pour les lignes\n",
    "#     \"axes.grid\": True,  # Activer la grille\n",
    "#     \"axes.grid.axis\": \"both\",  # Grille pour les deux axes (x et y)\n",
    "#     \"axes.grid.which\": \"major\",  # Grille pour les ticks principaux\n",
    "#     \"grid.color\": \"gray\",  # Couleur des lignes de la grille\n",
    "#     \"grid.linewidth\": 0.5,  # Épaisseur des lignes de la grille\n",
    "#     \"grid.alpha\": 0.6,  # Transparence des lignes de la grille\n",
    "\n",
    "#     # Ticks (Graduations)\n",
    "#     \"xtick.color\": \"white\",  # Couleur des ticks sur l'axe x\n",
    "#     \"ytick.color\": \"white\",  # Couleur des ticks sur l'axe y\n",
    "#     \"xtick.labelsize\": 16,  # Taille de la police des ticks sur l'axe x\n",
    "#     \"ytick.labelsize\": 16,  # Taille de la police des ticks sur l'axe y\n",
    "#     \"xtick.direction\": \"in\",  # Ticks pointant vers l'intérieur sur l'axe x\n",
    "#     \"ytick.direction\": \"in\",  # Ticks pointant vers l'intérieur sur l'axe y\n",
    "#     \"xtick.major.size\": 8,  # Longueur des ticks principaux sur l'axe x\n",
    "#     \"ytick.major.size\": 8,  # Longueur des ticks principaux sur l'axe y\n",
    "#     \"xtick.minor.size\": 4,  # Longueur des ticks secondaires sur l'axe x\n",
    "#     \"ytick.minor.size\": 4,  # Longueur des ticks secondaires sur l'axe y\n",
    "#     \"xtick.major.width\": 1.5,  # Épaisseur des ticks principaux sur l'axe x\n",
    "#     \"ytick.major.width\": 1.5,  # Épaisseur des ticks principaux sur l'axe y\n",
    "\n",
    "#     # Lignes et marqueurs\n",
    "#     # \"lines.linewidth\": 2,  # Épaisseur par défaut des lignes\n",
    "#     # \"lines.linestyle\": \"-\",  # Style par défaut des lignes continues\n",
    "#     # \"lines.color\": \"#1f77b4\",  # Couleur par défaut des lignes\n",
    "#     # \"lines.marker\": \"o\",  # Marqueur par défaut : cercle\n",
    "#     # \"lines.markersize\": 8,  # Taille par défaut des marqueurs\n",
    "#     # \"lines.markeredgewidth\": 1.5,  # Épaisseur du bord des marqueurs\n",
    "#     # \"lines.markerfacecolor\": \"blue\",  # Couleur du remplissage des marqueurs\n",
    "\n",
    "#     # Polices\n",
    "#     \"font.size\": 12,  # Taille globale de la police\n",
    "#     \"font.family\": \"sans-serif\",  # Famille de polices par défaut\n",
    "#     \"font.sans-serif\": [\"Arial\", \"Helvetica\", \"DejaVu Sans\"],  # Liste des polices sans-serif préférées\n",
    "#     \"text.color\": \"white\",  # Couleur du texte\n",
    "\n",
    "#     # Légendes\n",
    "#     \"legend.loc\": \"upper right\",  # Emplacement par défaut de la légende\n",
    "#     \"legend.fontsize\": 12,  # Taille de la police pour la légende\n",
    "#     \"legend.frameon\": True,  # Activer le cadre autour de la légende\n",
    "#     \"legend.framealpha\": 0.8,  # Transparence du cadre de la légende\n",
    "#     \"legend.edgecolor\": \"white\",  # Couleur de la bordure de la légende\n",
    "#     \"legend.facecolor\": (0.2, 0.2, 0.2, 0.9),  # Fond de la légende : gris foncé semi-transparent\n",
    "\n",
    "#     # Sauvegarde des graphiques\n",
    "#     \"savefig.dpi\": 300,  # Résolution par défaut pour les fichiers sauvegardés\n",
    "#     \"savefig.format\": \"png\",  # Format par défaut pour les fichiers sauvegardés\n",
    "#     \"savefig.facecolor\": (0, 0, 0, 1),  # Fond des figures sauvegardées : noir pur\n",
    "#     \"savefig.edgecolor\": \"none\",  # Pas de bordure pour les figures sauvegardées\n",
    "#     \"savefig.transparent\": True,  # Fond transparent pour les fichiers sauvegardés\n",
    "\n",
    "#     # Couleurs et cycles\n",
    "#     \"image.cmap\": \"viridis\",  # Palette par défaut pour les images\n",
    "# })\n",
    "\n",
    "plt.rcParams.update({\n",
    "    # Figure\n",
    "    \"figure.figsize\": (7, 5),  # Taille classique pour une figure d'article\n",
    "    \"figure.dpi\": 300,\n",
    "    \"figure.facecolor\": \"white\",\n",
    "    \"figure.edgecolor\": \"white\",\n",
    "    \"figure.titlesize\": 14,\n",
    "    \"figure.titleweight\": \"bold\",\n",
    "\n",
    "    # Axes\n",
    "    \"axes.facecolor\": \"white\",\n",
    "    \"axes.edgecolor\": \"black\",\n",
    "    \"axes.linewidth\": 1,\n",
    "    \"axes.titlesize\": 12,\n",
    "    \"axes.titleweight\": \"bold\",\n",
    "    \"axes.labelsize\": 11,\n",
    "    \"axes.labelweight\": \"normal\",\n",
    "    \"axes.labelcolor\": \"black\",\n",
    "    \"axes.prop_cycle\": cycler(color=[\n",
    "        \"#1f77b4\", \"#ff7f0e\", \"#2ca02c\", \"#d62728\", \"#9467bd\", \"#8c564b\"\n",
    "    ]),\n",
    "    \"axes.grid\": False,\n",
    "\n",
    "    # Ticks\n",
    "    \"xtick.color\": \"black\",\n",
    "    \"ytick.color\": \"black\",\n",
    "    \"xtick.labelsize\": 10,\n",
    "    \"ytick.labelsize\": 10,\n",
    "    \"xtick.direction\": \"out\",\n",
    "    \"ytick.direction\": \"out\",\n",
    "    \"xtick.major.size\": 5,\n",
    "    \"ytick.major.size\": 5,\n",
    "    \"xtick.major.width\": 1,\n",
    "    \"ytick.major.width\": 1,\n",
    "\n",
    "    # Lignes\n",
    "    \"lines.linewidth\": 1.5,\n",
    "    \"lines.markersize\": 6,\n",
    "\n",
    "    # Police\n",
    "    \"font.size\": 11,\n",
    "    \"font.family\": \"sans-serif\",\n",
    "    \"font.sans-serif\": [\"Arial\", \"Helvetica\", \"DejaVu Sans\"],\n",
    "    \"text.color\": \"black\",\n",
    "\n",
    "    # Légendes\n",
    "    \"legend.loc\": \"best\",\n",
    "    \"legend.fontsize\": 10,\n",
    "    \"legend.frameon\": False,\n",
    "\n",
    "    # Sauvegarde\n",
    "    \"savefig.dpi\": 600,\n",
    "    \"savefig.format\": \"png\",\n",
    "    \"savefig.facecolor\": \"white\",\n",
    "    \"savefig.edgecolor\": \"white\",\n",
    "    \"savefig.transparent\": False,\n",
    "\n",
    "    # Images\n",
    "    \"image.cmap\": \"viridis\",\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "<span style=\"color: skyblue; font-size: 20px; font-style: bold\">Initialisation des variables et constantes de travail..</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set initial time\n",
    "INITIAL_TIME = time.time()\n",
    "\n",
    "# Experiment parameters\n",
    "TIME_FRAME = 15\n",
    "SIZE_PIX = 0.637\n",
    "FPS = 1 / TIME_FRAME\n",
    "\n",
    "# File to study\n",
    "file_name = 'filtered_final'\n",
    "N_FRAME_MIN_STUDY = 200\n",
    "\n",
    "# Study parameters\n",
    "ROLLING_MEAN = False\n",
    "PIXELISATION = False\n",
    "TIME_FRAME_STUDY = False\n",
    "DRIFT = False\n",
    "\n",
    "# Plot parameters\n",
    "IMG_TYPE = 'png'\n",
    "ALPHA = 0.5\n",
    "LINEWIDTH = 0.1\n",
    "COLOR_SUP = 'blue'\n",
    "COLOR_INF = 'red'\n",
    "color_sup_inf = (COLOR_SUP, COLOR_INF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "<span style=\"color: skyblue; font-size: 20px; font-style: bold\">Définition des path et dossiers de travails / enregistrements.</span>\n",
    "\n",
    "<span style=\"color: skyblue; font-size: 20px; font-style: bold\">Conditions des manips étudiées.</span>\n",
    "\n",
    "<span style=\"color: skyblue; font-size: 20px; font-style: bold\">Défintions et créations des différents dossiers d'enregistrements.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General paths\n",
    "GENERAL_PATH = '/Users/souchaud/Desktop/Analyses/'\n",
    "GENERAL_PATH_PICTURES = '/Users/souchaud/Desktop/A_analyser/'\n",
    "\n",
    "# Condition\n",
    "CONDITION_simple = 'CytoOne_HL5_10x'\n",
    "CONDITION = f'{CONDITION_simple}_results_tracking'\n",
    "\n",
    "# Get list of experiments\n",
    "PATHWAY_EXPERIMENT = [f for f in os.listdir(GENERAL_PATH + CONDITION)\n",
    "                      if os.path.isdir(os.path.join(GENERAL_PATH + CONDITION, f))]\n",
    "\n",
    "# Update experiment paths\n",
    "PATHWAY_EXPERIMENT = [os.path.join(GENERAL_PATH, CONDITION, elem, 'mosaic')\n",
    "                      for elem in PATHWAY_EXPERIMENT]\n",
    "\n",
    "# Path to save pictures\n",
    "path_save_pic = os.path.join(GENERAL_PATH, f'résultats_{CONDITION}_All')\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(path_save_pic, exist_ok=True)\n",
    "os.chdir(path_save_pic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: skyblue; font-size: 20px; font-style: bold\">Ajout des temps d'incubation.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add incubation times to DATA\n",
    "conditions_to_values = {\n",
    "    'ASMOT127': 4.25, 'ASMOT128': 23.58, 'ASMOT130': 29.58, 'ASMOT132': 4.33, 'ASMOT133': 23.12,\n",
    "    'ASMOT134': 26.12, 'ASMOT135': 29, 'ASMOT136': 31.12, 'ASMOT137': 47.3, 'ASMOT138': 49.75,\n",
    "    'ASMOT139': 52.25, 'ASMOT140': 0, 'ASMOT141': 4.08, 'ASMOT142': 16.25, 'ASMOT143': 18.67,\n",
    "    'ASMOT144': 21.08, 'ASMOT145': 23.83, 'ASMOT146': 42.17, 'ASMOT147': 51.17, 'ASMOT148': 47.17,\n",
    "    'ASMOT149': 70.92, 'ASMOT150': 66.67, 'ASMOT151': 71.17, 'ASMOT152': 76.33, 'ASMOT153': 78.33,\n",
    "    'ASMOT154': 94.75, 'ASMOT155': 98.42, 'ASMOT156': 100.25, 'ASMOT157': 46.25, 'ASMOT158': 48.08,\n",
    "    'ASMOT159': 69.58, 'ASMOT160': 20.25, 'ASMOT161': 22.42, 'ASMOT162': 92.33, 'ASMOT163': 0.0,\n",
    "    'ASMOT164': 5.25, 'ASMOT165': 23.08, 'ASMOT166': 20.08, 'ASMOT167': 21.08, 'ASMOT168': 21.08,\n",
    "    'ASMOT169': 21.08, 'ASMOT170': 4.58, 'ASMOT171': 7.00, 'ASMOT172': 24.00, 'ASMOT173': 25.58,\n",
    "    'ASMOT174': 30.58, 'ASMOT175': 47.58, 'ASMOT176': 71.83, 'ASMOT177': 76.67, 'ASMOT178': 77.67,\n",
    "    'ASMOT179': 95.08, 'ASMOT180': 97.58, 'ASMOT181': 21.08,\n",
    "}\n",
    "\n",
    "experiment_to_dell= {\n",
    "    'ASMOT163',\n",
    "    'ASMOT161',\n",
    "    'ASMOT164',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "source": [
    "##\n",
    "<center><span style=\"color: Crimson; font-size: 30px; font-style: bold\">Lecture des données expériementales.</span></center>\n",
    "\n",
    "<span style=\"color: skyblue; font-size: 20px\">On décide de travailler que sur un certain nombre de frame. Ici je décide de travailler sur les 340 première frames pour normaliser les expériences. \n",
    "Donc la cellules doit être suivi sur N_MIN_STUDY sur les 340 premières frames. </span>\n",
    "\n",
    "<span style=\"color: skyblue; font-size: 20px\">Application de fonctions pour moyenne flissante et pixelisation et étude d'une frame sur x </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read HDF5 data\n",
    "importlib.reload(lib)\n",
    "DATA = lib.read_hdf5_all(\n",
    "    pathway_experiment=PATHWAY_EXPERIMENT,\n",
    "    name_file=file_name,\n",
    "    nbr_frame_min=N_FRAME_MIN_STUDY,\n",
    "    condition=CONDITION,\n",
    "    drift=DRIFT,\n",
    "    search_range=20,\n",
    "    memory=5\n",
    ")\n",
    "# Vérifier si le dictionnaire `experiment_to_dell` n'est pas vide\n",
    "if experiment_to_dell:\n",
    "    print(f\"Suppression des expériences : {experiment_to_dell}\")\n",
    "    \n",
    "    # Supprimer les expériences spécifiées\n",
    "    DATA = DATA[~DATA['experiment'].isin(experiment_to_dell)]\n",
    "    DATA.reset_index(drop=True, inplace=True)\n",
    "    print(f\"Nombre d'expériences restantes après suppression des expériences ratées: {DATA['experiment'].nunique()}\")\n",
    "else:\n",
    "    print(\"Aucune expérience à supprimer, `experiment_to_dell` est vide.\")\n",
    "\n",
    "# Sort DATA by 'frame'\n",
    "DATA.sort_values(by='frame', inplace=True)\n",
    "DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Longueur de DATA après tri sur la vitesse : {len(DATA)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE_PIX = 0.637      # µm/pixel\n",
    "TIME_FRAME = 15       # secondes par frame\n",
    "SPEED_THRESHOLD_UM_PER_MIN = 15\n",
    "SPEED_THRESHOLD_UM_PER_S = SPEED_THRESHOLD_UM_PER_MIN / 60  # ≈ 0.333 µm/s\n",
    "\n",
    "def compute_speed_filter(df):\n",
    "    # Tri par particule et frame\n",
    "    df = df.sort_values(['particle', 'frame']).copy()\n",
    "    dx = df.groupby('particle')['x'].diff()\n",
    "    dy = df.groupby('particle')['y'].diff()\n",
    "    dt = df.groupby('particle')['frame'].diff() * TIME_FRAME  # en secondes\n",
    "    dist_um = np.sqrt((dx * SIZE_PIX) ** 2 + (dy * SIZE_PIX) ** 2)\n",
    "    speed_um_s = dist_um / dt\n",
    "    speed_um_s = speed_um_s.fillna(0)\n",
    "    df['speed_um_s'] = speed_um_s\n",
    "    df['is_suspect'] = df['speed_um_s'] > SPEED_THRESHOLD_UM_PER_S\n",
    "    df_clean = df[~df['is_suspect']].copy()\n",
    "    return df_clean, df\n",
    "\n",
    "def speed_filter_iterative(df, speed_thresh_um_s, max_iter=10):\n",
    "    for i in range(max_iter):\n",
    "        df_clean, df_with_speed = compute_speed_filter(df)\n",
    "        n_removed = len(df) - len(df_clean)\n",
    "        print(f\"Iteration {i+1}: removed {n_removed} points\")\n",
    "        if n_removed == 0:\n",
    "            break\n",
    "        df = df_clean\n",
    "    return df\n",
    "\n",
    "print(f\"Longueur de DATA avant tri sur la vitesse : {len(DATA)}\")\n",
    "print(f\"avec {DATA['particle'].nunique()} particules\")\n",
    "# Utilisation :\n",
    "DATA = speed_filter_iterative(DATA, SPEED_THRESHOLD_UM_PER_S)\n",
    "print(f\"Longueur de DATA après tri sur la vitesse : {len(DATA)}\")\n",
    "print(f\"avec {DATA['particle'].nunique()} particules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def split_trajectories(df, gap_thresh=1):\n",
    "#     df = df.sort_values(['particle', 'frame']).copy()\n",
    "#     # Prépare une colonne pour les nouveaux IDs\n",
    "#     new_particles = []\n",
    "#     new_id = 0\n",
    "\n",
    "#     for pid, group in df.groupby('particle'):\n",
    "#         frames = group['frame'].to_numpy()\n",
    "#         # Calcule les gaps entre frames\n",
    "#         diffs = np.diff(frames, prepend=frames[0])\n",
    "#         current_id = new_id\n",
    "#         for i, gap in enumerate(diffs):\n",
    "#             if i == 0 or gap > gap_thresh:\n",
    "#                 current_id = new_id\n",
    "#                 new_id += 1\n",
    "#             new_particles.append(current_id)\n",
    "#     df['particle'] = new_particles\n",
    "#     return df\n",
    "\n",
    "# # 1. Découpe les trajectoires à chaque trou (trackpy attribue un nouvel ID à chaque fragment consécutif)\n",
    "# DATA = split_trajectories(DATA, gap_thresh=1)\n",
    "\n",
    "# # 2. (optionnel mais très conseillé) : Ne garde que les fragments d'au moins X frames\n",
    "# traj_lengths = DATA.groupby('particle')['frame'].count()\n",
    "# min_frames = 5  # par exemple\n",
    "# particles_to_keep = traj_lengths[traj_lengths >= min_frames].index\n",
    "# DATA = DATA[DATA['particle'].isin(particles_to_keep)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter DATA\n",
    "print(\"Nombre de particules avant tri: \", DATA['particle'].nunique())\n",
    "DATA = DATA[DATA['frame'] < 340]\n",
    "\n",
    "# Keep particles with sufficient frames\n",
    "DATA = DATA.groupby('particle').filter(lambda x: len(x) >= N_FRAME_MIN_STUDY)\n",
    "print(\"Nombre de particules après tri: \", DATA['particle'].nunique())\n",
    "\n",
    "# Apply optional data transformations\n",
    "if ROLLING_MEAN:\n",
    "    DATA = lib.rolling_mean(datas=DATA, roll=3)\n",
    "if PIXELISATION:\n",
    "    DATA = lib.pixelisation(datas=DATA, size_pix=SIZE_PIX)\n",
    "if TIME_FRAME_STUDY:\n",
    "    DATA, TIME_FRAME = lib.keep_nth_image(traj=DATA, n=N_FRAME_MIN_STUDY, time_frame=TIME_FRAME)\n",
    "\n",
    "# Calculate instant velocities\n",
    "DATA['time (min)'] = DATA['frame'] * TIME_FRAME / 60\n",
    "DATA = lib.vit_instant_new(traj=DATA, lag_time=TIME_FRAME, pix_size=SIZE_PIX, triage=1)\n",
    "\n",
    "DATA['time to incubation (hours)'] = DATA['experiment'].map(conditions_to_values).fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supposons que DATA soit ton DataFrame filtré\n",
    "# On compte le nombre de frames pour chaque particule\n",
    "traj_lengths = DATA.groupby('particle')['frame'].count()\n",
    "\n",
    "# Histogramme\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(traj_lengths, bins=50, alpha=0.7)\n",
    "plt.xlabel('Nombre de frames suivies')\n",
    "plt.ylabel('Nombre de cellules (trajectoires)')\n",
    "plt.title('Durée de suivi des cellules (après filtrage)')\n",
    "plt.show()\n",
    "\n",
    "# Optionnel : statistiques\n",
    "print(\"Statistiques des durées de suivi (en frames) :\")\n",
    "print(traj_lengths.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Calcul du nombre de frames par particle\n",
    "traj_lengths = DATA.groupby('particle')['frame'].count()\n",
    "\n",
    "# 2. Garder uniquement les particles avec >= 200 frames\n",
    "particles_to_keep = traj_lengths[traj_lengths >= 200].index\n",
    "\n",
    "# 3. Filtrer le DataFrame\n",
    "DATA_filtered = DATA[DATA['particle'].isin(particles_to_keep)].copy()\n",
    "\n",
    "print(f\"Nombre de trajectories gardées : {len(particles_to_keep)}\")\n",
    "print(f\"Nombre total de points après filtrage : {len(DATA_filtered)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA =DATA_filtered\n",
    "colonnes_a_garder = [\n",
    "    'y', 'x', 'mass', 'size', 'ecc', 'signal', 'raw_mass', 'ep',\n",
    "    'frame', 'old_particle', 'count_x', 'experiment', 'condition',\n",
    "    'count_y', 'particle', 'position', 'time (min)',\n",
    "    'time to incubation (hours)'\n",
    "]\n",
    "\n",
    "DATA = DATA[colonnes_a_garder]\n",
    "DATA = lib.vit_instant_new(traj=DATA, lag_time=TIME_FRAME, pix_size=SIZE_PIX, triage=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On filtre juste la colonne 'VitInst [um/min]' :\n",
    "# (Mettre à NaN les vitesses > 20)\n",
    "DATA['VitInst [um/min]'] = DATA['VitInst [um/min]'].where(\n",
    "    (DATA['VitInst [um/min]'] < 20) | (DATA['VitInst [um/min]'].isna()), np.nan\n",
    ")\n",
    "\n",
    "# Exemples de stats, NaN seront automatiquement ignorés\n",
    "mean_speed = DATA['VitInst [um/min]'].mean()\n",
    "median_speed = DATA['VitInst [um/min]'].median()\n",
    "print(f\"Mean speed: {mean_speed:.2f} µm/min, Median speed: {median_speed:.2f} µm/min\")\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.hist(DATA['VitInst [um/min]'].dropna(), bins=500, alpha=0.7, density=True)\n",
    "plt.title(\"Distribution des vitesses instantanées (<20 µm/min)\")\n",
    "plt.xlabel(\"VitInst [um/min]\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA.columns\n",
    "test = DATA['displacement [pix]']*0.637\n",
    "test = test / DATA['time (min)']\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.hist(test, bins=10, alpha=0.7, density=True)\n",
    "plt.title(\"Distribution des vitesses instantanées (<20 µm/min)\")\n",
    "plt.xlabel(\"VitInst [um/min]\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA['experiment'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # === PARAMÈTRES ===\n",
    "# base_image_dir = \"/Users/souchaud/Desktop/A_analyser/CytoOne_HL5_10x_faits/\"\n",
    "# output_dir = \"/Users/souchaud/Desktop/faibles/\"\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# # 1. Calcul de la première position de chaque particule\n",
    "# first_positions = DATA.sort_values('frame').groupby('particle', as_index=False).first()\n",
    "\n",
    "# # 2. Génération des images par expérience avec annotation des premières positions\n",
    "# for experiment in first_positions['experiment'].unique():\n",
    "#     sub_df = first_positions[first_positions['experiment'] == experiment].copy()\n",
    "\n",
    "#     # Localise le dossier de l’expérience\n",
    "#     experiment_dirs = glob.glob(os.path.join(base_image_dir, f\"*{experiment}*\"))\n",
    "#     if not experiment_dirs:\n",
    "#         print(f\"Dossier pour {experiment} non trouvé !\")\n",
    "#         continue\n",
    "#     experiment_dir = experiment_dirs[0]\n",
    "\n",
    "#     # Charge l'image frame 0\n",
    "#     img_path = os.path.join(experiment_dir, \"mosaic/mosaic_total_0.tif\")\n",
    "#     if not os.path.isfile(img_path):\n",
    "#         print(f\"Image {img_path} non trouvée !\")\n",
    "#         continue\n",
    "\n",
    "#     img = cv2.imread(img_path)\n",
    "#     if img is None:\n",
    "#         print(f\"Image {img_path} illisible !\")\n",
    "#         continue\n",
    "\n",
    "#     # Trace les cercles sur chaque première position\n",
    "#     for _, row in sub_df.iterrows():\n",
    "#         x = int(round(row[\"x\"]))\n",
    "#         y = int(round(row[\"y\"]))\n",
    "#         cv2.circle(img, (x, y), 50, (0, 0, 255), 3)\n",
    "\n",
    "#     # Sauvegarde\n",
    "#     out_path = os.path.join(output_dir, f\"{experiment}_mosaic_total_0_first_positions.png\")\n",
    "#     cv2.imwrite(out_path, img)\n",
    "#     print(f\"Image annotée sauvegardée : {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# === 0. Clustering sur mass, size, ecc ===\n",
    "X = DATA[['mass', 'size', 'ecc']].dropna()\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(X_scaled)\n",
    "DATA['cluster'] = -1\n",
    "DATA.loc[X.index, 'cluster'] = kmeans.labels_\n",
    "\n",
    "# === Affichage du clustering (facultatif) ===\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(\n",
    "    data=DATA, x='size', y='mass',\n",
    "    hue='cluster', palette=['green', 'orange'],\n",
    "    alpha=0.1, s=0.001, marker='+', linewidth=1.0\n",
    ")\n",
    "plt.title(\"K-means clustering (2 groupes)\")\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Préparation et entourer les particules avec un low MSD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # === PARAMÈTRES ===\n",
    "# base_image_dir = \"/Users/souchaud/Desktop/A_analyser/CytoOne_HL5_10x_faits/\"\n",
    "# output_dir = \"/Users/souchaud/Desktop/faibles/\"\n",
    "# min_distance = 10  # en pixels pour filtrage spatial\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# # === 1. Extraire particules du cluster 1 uniquement ===\n",
    "# data_cluster1 = DATA[DATA['cluster'] == 1].copy()\n",
    "\n",
    "# # === 2. Trouver la première frame de chaque particule ===\n",
    "# first_frame_df = data_cluster1.sort_values('frame').groupby('particle', as_index=False).first()\n",
    "# first_frame_df['start_frame'] = first_frame_df['frame']  # juste pour clarté\n",
    "\n",
    "# # === 3. Filtrage spatial dans chaque frame de début ===\n",
    "# filtered_particles = []\n",
    "\n",
    "# for f in first_frame_df['start_frame'].unique():\n",
    "#     same_frame = first_frame_df[first_frame_df['start_frame'] == f].copy()\n",
    "    \n",
    "#     if len(same_frame) <= 1:\n",
    "#         filtered_particles.append(same_frame)\n",
    "#         continue\n",
    "\n",
    "#     coords = same_frame[['x', 'y']].to_numpy()\n",
    "#     nbrs = NearestNeighbors(n_neighbors=2).fit(coords)\n",
    "#     distances, _ = nbrs.kneighbors(coords)\n",
    "#     keep_mask = distances[:, 1] > min_distance\n",
    "#     filtered_particles.append(same_frame[keep_mask])\n",
    "\n",
    "# # === 4. Particules uniques à annoter ===\n",
    "# final_df = pd.concat(filtered_particles, ignore_index=True)\n",
    "\n",
    "# # === 5. Génération des images par expérience ===\n",
    "# for experiment in final_df['experiment'].unique():\n",
    "#     sub_df = final_df[final_df['experiment'] == experiment].copy()\n",
    "\n",
    "#     # Localise le dossier de l’expérience\n",
    "#     experiment_dirs = glob.glob(os.path.join(base_image_dir, f\"*{experiment}*\"))\n",
    "#     if not experiment_dirs:\n",
    "#         print(f\"Dossier pour {experiment} non trouvé !\")\n",
    "#         continue\n",
    "#     experiment_dir = experiment_dirs[0]\n",
    "\n",
    "#     # Charge l'image frame 0\n",
    "#     img_path = os.path.join(experiment_dir, \"mosaic/mosaic_total_0.tif\")\n",
    "#     if not os.path.isfile(img_path):\n",
    "#         print(f\"Image {img_path} non trouvée !\")\n",
    "#         continue\n",
    "\n",
    "#     img = cv2.imread(img_path)\n",
    "#     if img is None:\n",
    "#         print(f\"Image {img_path} illisible !\")\n",
    "#         continue\n",
    "\n",
    "#     # Trace les cercles\n",
    "#     for _, row in sub_df.iterrows():\n",
    "#         x = int(round(row[\"x\"]))\n",
    "#         y = int(round(row[\"y\"]))\n",
    "#         cv2.circle(img, (x, y), 50, (0, 0, 255), 3)\n",
    "\n",
    "#     # Sauvegarde\n",
    "#     out_path = os.path.join(output_dir, f\"{experiment}_mosaic_total_0_cluster1_filtered.png\")\n",
    "#     cv2.imwrite(out_path, img)\n",
    "#     print(f\"Image annotée sauvegardée : {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "<center><span style=\"color: skyblue; font-size: 20px\"> Mean Mass and size plot </span></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Mean Mass and Size per Manipulation\n",
    "def plot_mean_mass_size(DATA, path_save_pic, IMG_TYPE):\n",
    "    manips = DATA['experiment'].unique()\n",
    "    num_manips = len(manips)\n",
    "    fig = plt.figure(figsize=(16, 2 * num_manips))\n",
    "    gs = gridspec.GridSpec(num_manips, 3, fig)\n",
    "    colors = ['skyblue', 'lightgreen', 'salmon']\n",
    "    for i, manip in enumerate(manips):\n",
    "        data_manip = DATA[DATA['experiment'] == manip]\n",
    "        mass_means = data_manip.groupby('particle')['mass'].mean()\n",
    "        size_means = data_manip.groupby('particle')['size'].mean()\n",
    "        filtered_data = data_manip[data_manip['frame'] == 0]\n",
    "\n",
    "        ax1 = fig.add_subplot(gs[i, 0])\n",
    "        ax1.hist(mass_means, bins=100, color=colors[0], density=True)\n",
    "        ax1.set_title(f\"Mean mass of particles for {manip}\")\n",
    "        ax1.set_xlabel(\"Mean mass\")\n",
    "        ax1.set_ylabel(\"Density\")\n",
    "\n",
    "        ax2 = fig.add_subplot(gs[i, 1])\n",
    "        ax2.hist(size_means, bins=100, color=colors[1], density=True)\n",
    "        ax2.set_title(f\"Mean size of particles for {manip}\")\n",
    "        ax2.set_xlabel(\"Mean size\")\n",
    "        ax2.set_ylabel(\"Density\")\n",
    "\n",
    "        ax3 = fig.add_subplot(gs[i, 2])\n",
    "        ax3.scatter(filtered_data['size'], filtered_data['mass'], c=\"#d62728\", edgecolors=\"#d62728\", alpha=0.7)\n",
    "        ax3.set_title(f\"Mass vs. Size at frame 0 for {manip}\")\n",
    "        ax3.set_xlabel(\"Size\")\n",
    "        ax3.set_ylabel(\"Mass\")\n",
    "        ax3.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    fig.savefig(os.path.join(path_save_pic, f\"Mean_Mass_Size_manip.{IMG_TYPE}\"), format=IMG_TYPE)\n",
    "\n",
    "plot_mean_mass_size(DATA, path_save_pic, IMG_TYPE='svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "<center><span style=\"color: skyblue; font-size: 20px\"> plot total path for each partciel in each experiment (histograms) </span></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total path for the first N frames\n",
    "path_data = lib.calculate_total_path_first_frames(DATA, first_n_frames=100)\n",
    "\n",
    "# Plot Total Path in First 100 Frames per Experiment\n",
    "def plot_total_path(path_data):\n",
    "    grouped = path_data.groupby('experiment')\n",
    "    n_experiments = len(grouped)\n",
    "    fig, axes = plt.subplots(nrows=n_experiments, figsize=(10, 2 * n_experiments))\n",
    "    axes = axes if n_experiments > 1 else [axes]\n",
    "    for (experiment, group), ax in zip(grouped, axes):\n",
    "        ax.hist(group['total_path_first_n'], bins=50, range=[0, 100], density=True, alpha=0.5)\n",
    "        ax.set_title(f\"Total path in first 100 frames for {experiment}\")\n",
    "        ax.set_xlabel('Length path (μm)')\n",
    "        ax.set_ylabel('Density')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(path_save_pic, f\"Total path in first 100 frames.{IMG_TYPE}\"), format=IMG_TYPE)\n",
    "    plt.show()\n",
    "\n",
    "plot_total_path(path_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "<center><span style=\"color: skyblue; font-size: 20px\"> Creation des traj centrées / distance cumulée / IMSD </span></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Center trajectories\n",
    "DATA.reset_index(inplace=True)\n",
    "DATA = lib.center(traj=DATA)\n",
    "\n",
    "print(f\"\\n\\nTemps de préparation des données pour {CONDITION}: {time.time() - INITIAL_TIME} sec\\n\\n\")\n",
    "\n",
    "# Calculate total and cumulative displacement\n",
    "DATA, start_end = lib.length_displacement(traj=DATA, size_pix=SIZE_PIX)\n",
    "\n",
    "# Compute MSD and cutoff\n",
    "DATA2 = DATA.copy()\n",
    "DATA2['frame'] = pd.factorize(DATA2['frame'])[0]\n",
    "IMSD = tp.imsd(traj=DATA2, mpp=SIZE_PIX, fps=FPS, max_lagtime=200, statistic='msd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "<center><span style=\"color: skyblue; font-size: 20px\"> Premier fit pour exclure certaines traj </span></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trajectory clustering with fit and defining a cutoff\n",
    "LAG_TIME_FIT = 5\n",
    "importlib.reload(lib)\n",
    "COEF_INF, COEF_SUP, PART_COEF_INF, PART_COEF_SUP, CUTOFF = lib.traj_clustering_with_fit_cutoff(\n",
    "    DATA2, imsd=IMSD, hist=True, lag_time_fit=LAG_TIME_FIT, micronperpixel=SIZE_PIX,\n",
    "    fps=FPS, binsize=250, peak_height=50, peak_width=1, save=True, pathway_fig=path_save_pic,\n",
    "    name='all_experiment_autocorr', img_type=IMG_TYPE, plot=True, color_sup_inf=color_sup_inf,\n",
    "    cutoff_default=0\n",
    ")\n",
    "\n",
    "# Keep only particles above cutoff\n",
    "DATA = DATA[DATA['particle'].isin(PART_COEF_SUP)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = DATA[DATA['VitInst [um/min]'] < 20]\n",
    "DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "<center><span style=\"color: skyblue; font-size: 20px\"> Nombre de particules par frame pour les expériences. </span></center>\n",
    "\n",
    "ça n'a pas un grand interêt ici, mais c'est par principe pour vérification et compréhension d'éventuels phenomènes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Number of Particles per Frame for All Experiments in One Graph\n",
    "def plot_nbr_particles_per_frame_combined(DATA, path_save_pic, IMG_TYPE):\n",
    "    \"\"\"\n",
    "    Plot the number of unique particles per frame for all experiments on a single graph with different colors.\n",
    "\n",
    "    Parameters:\n",
    "    - DATA (DataFrame): The input data containing tracking information.\n",
    "    - path_save_pic (str): The path where the plot will be saved.\n",
    "    - IMG_TYPE (str): The format for saving the plot (e.g., 'png', 'jpg').\n",
    "    \"\"\"\n",
    "    experiments = DATA['experiment'].unique()\n",
    "\n",
    "    # Generate a colormap with enough unique colors for all experiments\n",
    "    colormap = plt.colormaps['tab20']  # Use the modern colormap API\n",
    "    colors = [colormap(i / len(experiments)) for i in range(len(experiments))]\n",
    "\n",
    "    plt.figure(figsize=(12, 13))\n",
    "    for i, exp in enumerate(experiments):\n",
    "        # Group data by time and calculate the number of unique particles per frame\n",
    "        nbr_part_per_frame = DATA[DATA['experiment'] == exp].groupby('time (min)')['particle'].nunique()\n",
    "        plt.plot(\n",
    "            nbr_part_per_frame.index, nbr_part_per_frame.values, \n",
    "            label=exp, color=colors[i], alpha=1, linewidth=0.8\n",
    "        )\n",
    "\n",
    "    # Add labels, legend, and title\n",
    "    plt.title('Number of Particles per Frame for All Experiments', fontsize=14)\n",
    "    plt.xlabel('Time (min)', fontsize=12)\n",
    "    plt.ylabel('Number of Particles', fontsize=12)\n",
    "    plt.legend(title=\"Experiments\", bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save and show the plot\n",
    "    fig_path = os.path.join(path_save_pic, f\"Nbr_particle_per_Frame_combined.{IMG_TYPE}\")\n",
    "    plt.savefig(fig_path, format=IMG_TYPE, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Plot saved to {fig_path}\")\n",
    "\n",
    "# Appel de la fonction\n",
    "plot_nbr_particles_per_frame_combined(DATA, path_save_pic, IMG_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot Number of Particles per Frame per Experiment\n",
    "# def plot_nbr_particles_per_frame(DATA, path_save_pic, IMG_TYPE):\n",
    "#     experiments = DATA['experiment'].unique()\n",
    "#     n_cols = 2\n",
    "#     n_rows = (len(experiments) + n_cols - 1) // n_cols\n",
    "#     fig, axs = plt.subplots(n_rows, n_cols, figsize=(20 * n_cols, 10 * n_rows))\n",
    "#     axs = axs.flatten()\n",
    "#     for i, exp in enumerate(experiments):\n",
    "#         nbr_part_per_frame = DATA[DATA['experiment'] == exp].groupby('time (min)')['particle'].nunique()\n",
    "#         ax = axs[i]\n",
    "#         ax.plot(nbr_part_per_frame.index, nbr_part_per_frame.values)\n",
    "#         ax.set_ylim([0, nbr_part_per_frame.max() + 10])\n",
    "#         ax.set_title(f'Nbr particle per Frame - {exp}')\n",
    "#         ax.set_xlabel('Time (min)')\n",
    "#         ax.set_ylabel('Number of particles')\n",
    "#     for ax in axs[len(experiments):]:\n",
    "#         ax.axis('off')\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(os.path.join(path_save_pic, f\"Nbr_particle_per_Frame_manip_par_manip.{IMG_TYPE}\"), format=IMG_TYPE)\n",
    "#     plt.show()\n",
    "\n",
    "# plot_nbr_particles_per_frame(DATA, path_save_pic, IMG_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "<center><span style=\"color: skyblue; font-size: 20px\"> Nombre de particules par frame pour toutes les paticules. </span></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Number of Particles per Frame\n",
    "nbr_part_per_frame = DATA.groupby('time (min)')['particle'].nunique()\n",
    "lib.plot_datas(\n",
    "    x_values=nbr_part_per_frame.index,\n",
    "    y_values=nbr_part_per_frame.values,\n",
    "    title='Nbr particles per Frame',\n",
    "    x_label='Time (min)', y_label='Number of particles',\n",
    "    x_lim=[0, nbr_part_per_frame.index.max()], y_lim=[0, 10000],\n",
    "    save=True, path_save_pic=path_save_pic, img_type=IMG_TYPE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "<center><span style=\"color: skyblue; font-size: 40px\">Trajectoires toutes rassemblées. </span></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Trajectories after Removing Suspicious Particles\n",
    "fig, axis = plt.subplots(figsize=(12, 12), dpi=300)\n",
    "axis.set_aspect('equal', 'box')\n",
    "plt.title('Trajectories after removing suspicious particles', fontsize=16)\n",
    "tp.plot_traj(DATA, label=False, ax=axis)\n",
    "for line in axis.get_lines():\n",
    "    line.set_linewidth(0.1)\n",
    "plt.show()\n",
    "fig.savefig(os.path.join(path_save_pic, f'Trajectories_after_removing_suspicious_particles.{IMG_TYPE}'),\n",
    "            format=IMG_TYPE, dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot centered Trajectories after Removing Suspicious Particles\n",
    "fig, axis = plt.subplots(figsize=(12, 12), dpi=300)\n",
    "axis.set_aspect('equal', 'box')\n",
    "plt.title('Centered trajectories after removing suspicious particles', fontsize=16)\n",
    "tp.plot_traj(DATA[['Xc [pix]', 'Yc [pix]', 'frame', 'particle']].rename(columns={'Xc [pix]': 'x', 'Yc [pix]': 'y'}), label=False, ax=axis)\n",
    "for line in axis.get_lines():\n",
    "    line.set_linewidth(0.2)\n",
    "plt.show()\n",
    "fig.savefig(os.path.join(path_save_pic, f'Centered trajectories_after_removing_suspicious_particles.{IMG_TYPE}'),\n",
    "            format=IMG_TYPE, dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Trajectories on Original Frames\n",
    "def plot_trajectories_on_frames(DATA, path_save_pic, GENERAL_PATH_PICTURES, CONDITION_simple):\n",
    "    image_path = os.path.join(GENERAL_PATH_PICTURES, f\"{CONDITION_simple}_faits\")\n",
    "    plot_exp = DATA.groupby('experiment')\n",
    "    num_experiments = len(plot_exp)\n",
    "    num_cols = 2\n",
    "    num_rows = (num_experiments + num_cols - 1) // num_cols\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(20, num_rows * 5))\n",
    "    axes = axes.flatten()\n",
    "    for ax, (exp_name, exp_data) in zip(axes, plot_exp):\n",
    "        exp_directories = []\n",
    "        for dirpath, dirnames, _ in os.walk(image_path):\n",
    "            for dirname in dirnames:\n",
    "                if exp_name in dirname:\n",
    "                    full_path = os.path.join(dirpath, dirname)\n",
    "                    exp_directories.append(full_path)\n",
    "        if exp_directories:\n",
    "            image_path_directory = os.path.join(exp_directories[0], 'mosaic', 'mosaic_total_0.tif')\n",
    "            frame = imageio.imread(image_path_directory)\n",
    "            ax.set_aspect('equal', 'box')\n",
    "            ax.set_title(f'Trajectories for {exp_name}')\n",
    "            tp.plot_traj(exp_data, superimpose=frame, label=False, ax=ax)\n",
    "        else:\n",
    "            print(f\"No directory found for {exp_name}\")\n",
    "            ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    fig.savefig(os.path.join(path_save_pic, 'trajectories_on_frame_all_experiment.pdf'), format='pdf')\n",
    "\n",
    "plot_trajectories_on_frames(DATA, path_save_pic, GENERAL_PATH_PICTURES, CONDITION_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "<center><span style=\"color: skyblue; font-size: 20px\"> Vitesse instantanée moyenne par frame. </span></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Mean Instantaneous Speed per Frame\n",
    "mean_VitInst_per_frame = DATA.groupby('time (min)')['VitInst [um/min]'].median()\n",
    "mean_VitInst_per_frame = mean_VitInst_per_frame.rolling(5).mean().dropna()\n",
    "\n",
    "# Appel avec vérification du répertoire\n",
    "lib.plot_datas(\n",
    "    x_values=mean_VitInst_per_frame.index,\n",
    "    y_values=mean_VitInst_per_frame.values,\n",
    "    title='Mean VitInst [um/min] per Frame',\n",
    "    x_label='Time (min)', y_label='Mean VitInst [um/min]',\n",
    "    x_lim=[0, mean_VitInst_per_frame.index.max()], y_lim=[0, 10],\n",
    "    save=True, path_save_pic=path_save_pic, img_type=IMG_TYPE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recompute MSD\n",
    "# DATA_intermediaire = DATA.copy()\n",
    "DATA['frame'] = pd.factorize(DATA['frame'])[0]\n",
    "IMSD = tp.imsd(traj=DATA, mpp=SIZE_PIX, fps=FPS, max_lagtime=200, statistic='msd')\n",
    "\n",
    "lib.plot_msd(IMSD, fps=FPS, name=\"MSD of all frames vs lag time (s)\",\n",
    "             color_plot='forestgreen', save=True, pathway_saving=path_save_pic,\n",
    "             alpha=0.3, linewidth=0.1, img_type=IMG_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload custom library\n",
    "importlib.reload(lib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recompute trajectory clustering with new cutoff\n",
    "LAG_TIME_FIT = 5\n",
    "COEF_INF, COEF_SUP, PART_COEF_INF, PART_COEF_SUP, CUTOFF = lib.traj_clustering_with_fit_cutoff(\n",
    "    DATA, imsd=IMSD, hist=True, lag_time_fit_start=0, lag_time_fit=LAG_TIME_FIT , micronperpixel=SIZE_PIX,\n",
    "    fps=FPS, binsize=250, peak_height=50, peak_width=1, save=True, pathway_fig=path_save_pic,\n",
    "    name='MSD and slopes', img_type=IMG_TYPE, plot=True, color_sup_inf=color_sup_inf,\n",
    "    cutoff_default=0.6\n",
    ")\n",
    "\n",
    "# Ajouter les colonnes 'is_inf' et 'is_sup' à DATA\n",
    "DATA['particle'] = DATA['particle'].astype(int)\n",
    "PART_COEF_INF = set(map(int, PART_COEF_INF))\n",
    "PART_COEF_SUP = set(map(int, PART_COEF_SUP))\n",
    "\n",
    "# Ajout des colonnes 'is_inf' et 'is_sup'\n",
    "DATA['is_inf'] = DATA['particle'].isin(PART_COEF_INF).astype(int)\n",
    "DATA['is_sup'] = DATA['particle'].isin(PART_COEF_SUP).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pour éviter les effets de labels/marqueurs automatiques\n",
    "def plot_trajectories(data, color, ax, linewidth=0.2):\n",
    "    for pid, group in data.groupby('particle'):\n",
    "        ax.plot(group['Xc [pix]'], group['Yc [pix]'], color=color, lw=linewidth)\n",
    "\n",
    "fig, axis = plt.subplots(figsize=(12, 12), dpi=300)\n",
    "axis.set_aspect('equal', 'box')\n",
    "plt.title('Centered trajectories after removing suspicious particles', fontsize=16)\n",
    "\n",
    "# 2. Trajectoires bleues (PART_COEF_SUP)\n",
    "data_sup = DATA[DATA['particle'].isin(PART_COEF_SUP)]\n",
    "plot_trajectories(data_sup, color='blue', ax=axis, linewidth=0.2)\n",
    "\n",
    "# 1. Trajectoires rouges (PART_COEF_INF)\n",
    "data_inf = DATA[DATA['particle'].isin(PART_COEF_INF)]\n",
    "plot_trajectories(data_inf, color='red', ax=axis, linewidth=0.2)\n",
    "\n",
    "\n",
    "\n",
    "plt.xlabel('Xc [pix]')\n",
    "plt.ylabel('Yc [pix]')\n",
    "plt.show()\n",
    "\n",
    "# Enregistrement\n",
    "fig.savefig(\n",
    "    os.path.join(path_save_pic, f'Centered_trajectories_grouped.{IMG_TYPE}'),\n",
    "    format=IMG_TYPE, dpi=300, bbox_inches='tight'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_coef_sup = DATA[DATA['particle'].isin(PART_COEF_INF)]\n",
    "data_coef_sup['experiment'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot centered Trajectories after Removing Suspicious Particles\n",
    "fig, axis = plt.subplots(figsize=(12, 12), dpi=300)\n",
    "axis.set_aspect('equal', 'box')\n",
    "plt.title('Centered trajectories after removing suspicious particles', fontsize=16)\n",
    "data_inf = DATA[DATA['particle'].isin(PART_COEF_INF)]\n",
    "tp.plot_traj(data_inf[['Xc [pix]', 'Yc [pix]', 'frame', 'particle']].rename(columns={'Xc [pix]': 'x', 'Yc [pix]': 'y'}), label=False, ax=axis)\n",
    "for line in axis.get_lines():\n",
    "    line.set_linewidth(0.2)\n",
    "plt.show()\n",
    "# fig.savefig(os.path.join(path_save_pic, f'Centered trajectories_after_removing_suspicious_particles.{IMG_TYPE}'),\n",
    "#             format=IMG_TYPE, dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Nb de particules total :\", DATA['particle'].nunique())\n",
    "print(\"Nb particules INF demandées :\", len(PART_COEF_INF))\n",
    "print(\"Nb réellement trouvées :\", DATA[DATA['particle'].isin(PART_COEF_INF)]['particle'].nunique())\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "#  MSD MOYENNE : GLOBAL vs PART_COEF_INF vs PART_COEF_SUP\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "MAX_LAG = 200   # le même que précédemment (en frames)\n",
    "\n",
    "# --- 2) MSD moyenne des particules INF ------------------\n",
    "DATA_inf  = DATA[DATA['particle'].isin(PART_COEF_INF)]\n",
    "# --- 3) MSD moyenne des particules SUP ------------------\n",
    "DATA_sup  = DATA[DATA['particle'].isin(PART_COEF_SUP)]\n",
    "\n",
    "EMSD_inf = tp.emsd(traj=DATA[DATA['particle'].isin(PART_COEF_INF)], mpp=SIZE_PIX, fps=FPS, max_lagtime=MAX_LAG, detail=True)\n",
    "EMSD_sup = tp.emsd(traj=DATA[DATA['particle'].isin(PART_COEF_SUP)], mpp=SIZE_PIX, fps=FPS, max_lagtime=MAX_LAG, detail=True)\n",
    "EMSD = tp.emsd(traj=DATA, mpp=SIZE_PIX, fps=FPS, max_lagtime=MAX_LAG, detail=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifie que les DataFrames ont bien les colonnes attendues\n",
    "assert 'msd' in EMSD.columns, \"La colonne 'msd' est absente de EMSD\"\n",
    "assert 'msd' in EMSD_inf.columns, \"La colonne 'msd' est absente de EMSD_inf\"\n",
    "assert 'msd' in EMSD_sup.columns, \"La colonne 'msd' est absente de EMSD_sup\"\n",
    "\n",
    "# Tracé des MSD moyennes\n",
    "plt.figure(figsize=(7, 5))\n",
    "\n",
    "plt.plot(EMSD.index, EMSD['msd'], label='Global', color='black', linewidth=2)\n",
    "plt.plot(EMSD_inf.index, EMSD_inf['msd'], label='PART_COEF_INF', color='red', linewidth=1.5)\n",
    "plt.plot(EMSD_sup.index, EMSD_sup['msd'], label='PART_COEF_SUP', color='blue', linewidth=1.5)\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Lag time (s)')\n",
    "plt.ylabel('MSD (µm²)')\n",
    "plt.title('Mean MSD - Global vs INF vs SUP (via tp.emsd)')\n",
    "plt.legend()\n",
    "plt.grid(True, which='both', linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre de points à utiliser pour les fits\n",
    "N_points = 10\n",
    "\n",
    "# Préparer le plot\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "# Courbes à tracer\n",
    "courbes = {\n",
    "    # 'Global': EMSD,\n",
    "    'PART_COEF_INF': EMSD_inf,\n",
    "    'PART_COEF_SUP': EMSD_sup\n",
    "}\n",
    "\n",
    "# Couleurs associées\n",
    "couleurs = {\n",
    "    # 'Global': 'black',\n",
    "    'PART_COEF_INF': 'red',\n",
    "    'PART_COEF_SUP': 'blue'\n",
    "}\n",
    "\n",
    "# Tracer les courbes MSD + les fits\n",
    "for label, df in courbes.items():\n",
    "    lag = np.array(df['lagt'])\n",
    "    msd = np.array(df['msd'])\n",
    "\n",
    "    # Tracer la courbe principale\n",
    "    plt.plot(lag, msd, label=label, color=couleurs[label], linewidth=1)\n",
    "\n",
    "    # === Fit court temps ===\n",
    "    log_lag_start = np.log10(lag[:3])\n",
    "    log_msd_start = np.log10(msd[:3])\n",
    "    slope_start, intercept_start, _, _, _ = linregress(log_lag_start, log_msd_start)\n",
    "    fit_start = 10 ** (slope_start * np.log10(lag[:]) + intercept_start)\n",
    "\n",
    "    plt.plot(lag[:], fit_start, '--', color=couleurs[label],\n",
    "             linewidth=1.5, label=f\"{label} fit début\\nα={slope_start:.2f} and intercept={intercept_start:.2f}\")\n",
    "\n",
    "    # === Fit moyen temps ===\n",
    "    log_lag_end = np.log10(lag[5:12])\n",
    "    log_msd_end = np.log10(msd[5:12])\n",
    "    slope_end, intercept_end, _, _, _ = linregress(log_lag_end, log_msd_end)\n",
    "    fit_end = 10 ** (slope_end * np.log10(lag[:]) + intercept_end)\n",
    "\n",
    "    plt.plot(lag[:], fit_end, ':', color=couleurs[label],\n",
    "             linewidth=2, label=f\"{label} fit fin\\nα={slope_end:.2f} and intercept={intercept_end:.2f}\")\n",
    "\n",
    "# Mise en forme du graphe\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Lag time (s)', fontsize=14)\n",
    "plt.ylabel('MSD (µm²)', fontsize=14)\n",
    "plt.title('MSD + Régressions début et fin (log-log)', fontsize=16)\n",
    "plt.grid(True, which='both', ls='--', alpha=0.4)\n",
    "plt.legend(fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import linregress\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N_FIRST = 5  # fit points 0 à 4\n",
    "N_NEXT = 5   # fit points 5 à 9\n",
    "N_LAST = 100  # fit points -10 à la fin\n",
    "\n",
    "# --- FIT sur les 5 premiers points ---\n",
    "log_lag_first = np.log10(lag[:N_FIRST])\n",
    "log_msd_first = np.log10(msd[:N_FIRST])\n",
    "slope_first, intercept_first, *_ = linregress(log_lag_first, log_msd_first)\n",
    "fit_first = 10 ** (slope_first * np.log10(lag) + intercept_first)\n",
    "\n",
    "# --- FIT sur les points 6 à 10 (indices 5 à 9) ---\n",
    "log_lag_next = np.log10(lag[N_FIRST:N_FIRST+N_NEXT])\n",
    "log_msd_next = np.log10(msd[N_FIRST:N_FIRST+N_NEXT])\n",
    "slope_next, intercept_next, *_ = linregress(log_lag_next, log_msd_next)\n",
    "fit_next = 10 ** (slope_next * np.log10(lag) + intercept_next)\n",
    "\n",
    "# --- FIT sur les 10 derniers points ---\n",
    "log_lag_last = np.log10(lag[-N_LAST:])\n",
    "log_msd_last = np.log10(msd[-N_LAST:])\n",
    "slope_last, intercept_last, *_ = linregress(log_lag_last, log_msd_last)\n",
    "fit_last = 10 ** (slope_last * np.log10(lag) + intercept_last)\n",
    "\n",
    "# --- Tracé ---\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.plot(lag, msd, 'ko-', label='MSD brute')\n",
    "plt.plot(lag, fit_first, 'b--', label=f'Fit points 1-5\\nα={slope_first:.2f}')\n",
    "plt.plot(lag, fit_next, 'r--', label=f'Fit points 6-10\\nα={slope_next:.2f}')\n",
    "plt.plot(lag, fit_last, 'g--', label=f'Fit 10 derniers points\\nα={slope_last:.2f}')\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Lag time (s)')\n",
    "plt.ylabel('MSD (µm²)')\n",
    "plt.title('MSD brute + fits log-log courts')\n",
    "plt.grid(True, which='both', ls='--')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "#  FONCTION 1 : AUTOCORRÉLATION D’UNE SÉRIE 1-D\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "def autocorr_msd(msd: np.ndarray, max_lag: int = None) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calcule l'autocorrélation normalisée d'une série MSD (1-D).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    msd : np.ndarray\n",
    "        Valeurs MSD(t) pour un seul lag (1-D).\n",
    "    max_lag : int, optional\n",
    "        Nombre maximal de lags à considérer (en frames). Par défaut : N-1.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "        Autocorrélation normalisée C(τ) avec τ∈[0, max_lag] (index = lag).\n",
    "    \"\"\"\n",
    "    msd = np.asarray(msd, dtype=float)\n",
    "    N = len(msd)\n",
    "    if max_lag is None or max_lag >= N:\n",
    "        max_lag = N - 1\n",
    "\n",
    "    mu  = msd.mean()\n",
    "    var = msd.var()\n",
    "    if var == 0:\n",
    "        return pd.Series([1.0] + [0.0]*max_lag, index=range(max_lag + 1))\n",
    "\n",
    "    acov = np.array([\n",
    "        np.dot(msd[:N-lag] - mu, msd[lag:] - mu) / (N - lag)\n",
    "        for lag in range(max_lag + 1)\n",
    "    ])\n",
    "    acorr = acov / var\n",
    "    return pd.Series(acorr, index=range(max_lag + 1), name=\"autocorr_msd\")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "#  FONCTION 2 : AUTOCORRÉLATION À PARTIR DE trackpy.imsd\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "def msd_autocorrelation_from_imsd(imsd: pd.DataFrame, fps: float,\n",
    "                                  max_lag: int = None,\n",
    "                                  agg: str = 'mean') -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calcule l'autocorrélation normalisée de la MSD agrégée (mean/median)\n",
    "    à partir du DataFrame renvoyé par trackpy.imsd.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    imsd : pd.DataFrame\n",
    "        Sortie de trackpy.imsd (index = lag, colonnes = particules).\n",
    "    fps : float\n",
    "        Taux d'acquisition (images par seconde).\n",
    "    max_lag : int, optional\n",
    "        Nombre maximal de lags (en frames) pour l'autocorrélation.\n",
    "    agg : {'mean', 'median'}\n",
    "        Méthode d’agrégation des MSD sur les particules.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "        Autocorrélation normalisée indexée par le lag en secondes.\n",
    "    \"\"\"\n",
    "    if agg == 'mean':\n",
    "        msd_series = imsd.mean(axis=1).values\n",
    "    elif agg == 'median':\n",
    "        msd_series = imsd.median(axis=1).values\n",
    "    else:\n",
    "        raise ValueError(\"agg must be 'mean' or 'median'\")\n",
    "\n",
    "    ac = autocorr_msd(msd_series, max_lag=max_lag)\n",
    "\n",
    "    # Conversion index (frames) → secondes\n",
    "    ac.index = ac.index / fps\n",
    "    ac.index.name = \"lag_seconds\"\n",
    "    ac.name = f\"autocorr_msd_{agg}\"\n",
    "    return ac\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "#  EXEMPLE D’UTILISATION\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    # EXEMPLE : suppose que 'DATA' est votre DataFrame de trajectoires\n",
    "    # et que vous avez déjà défini SIZE_PIX et FPS.\n",
    "    #\n",
    "    # 1) Calcul MSD (trackpy)\n",
    "    IMSD = tp.imsd(traj=DATA,\n",
    "                   mpp=SIZE_PIX,   # micron par pixel\n",
    "                   fps=FPS,\n",
    "                   max_lagtime=200,\n",
    "                   statistic='msd')   # => DataFrame (lags × particules)\n",
    "\n",
    "    # 2) Autocorrélation normalisée (mean sur particules)\n",
    "    ac_msd = msd_autocorrelation_from_imsd(IMSD, fps=FPS,\n",
    "                                           max_lag=100, agg='mean')\n",
    "\n",
    "    # 3) Tracé\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(ac_msd.index, ac_msd.values, marker='o')\n",
    "    plt.xlabel(\"Lag (s)\")\n",
    "    plt.ylabel(\"Autocorrélation normalisée de la MSD\")\n",
    "    plt.title(\"Autocorrélation de la MSD (moyenne sur particules)\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "<center><span style=\"color: skyblue; font-size: 40px\">Calculs des différents metrics importants </span></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(DATA):\n",
    "    # Calculer les vitesses moyennes instantanées pour toutes les particules\n",
    "    mean_vitinst_df = DATA.groupby(['experiment', 'particle'])['VitInst [um/min]'].mean().reset_index()\n",
    "    mean_vitinst_df = mean_vitinst_df.rename(columns={'VitInst [um/min]': 'mean_vit_inst'})\n",
    "\n",
    "    # Calculer les médianes des vitesses instantanées pour toutes les particules\n",
    "    median_vitinst_df = DATA.groupby(['experiment', 'particle'])['VitInst [um/min]'].median().reset_index()\n",
    "    median_vitinst_df = median_vitinst_df.rename(columns={'VitInst [um/min]': 'median_vit_inst'})\n",
    "\n",
    "    # Initialisation d'une liste pour stocker les résultats\n",
    "    metrics = []\n",
    "\n",
    "    # Parcours de chaque particule pour calculer les métriques\n",
    "    for particle, group in DATA.groupby('particle'):\n",
    "        experiment = group['experiment'].iloc[0]\n",
    "\n",
    "        # Limiter aux 200 premières lignes\n",
    "        limited_group = group.head(200)\n",
    "\n",
    "        # Somme des déplacements (seulement sur les 200 premières lignes)\n",
    "        displacement_sum = limited_group['displacement [pix]'].sum()\n",
    "\n",
    "        # Récupérer la vitesse moyenne instantanée pré-calculée\n",
    "        mean_vit_inst = mean_vitinst_df.loc[\n",
    "            (mean_vitinst_df['experiment'] == experiment) & (mean_vitinst_df['particle'] == particle),\n",
    "            'mean_vit_inst'\n",
    "        ].values[0]\n",
    "\n",
    "        # Récupérer la médiane des vitesses instantanées pré-calculée\n",
    "        median_vit_inst = median_vitinst_df.loc[\n",
    "            (median_vitinst_df['experiment'] == experiment) & (median_vitinst_df['particle'] == particle),\n",
    "            'median_vit_inst'\n",
    "        ].values[0]\n",
    "\n",
    "        # Distance entre la position de départ et d'arrivée\n",
    "        start_position = limited_group.iloc[0][['x', 'y']].values\n",
    "        end_position = limited_group.iloc[-1][['x', 'y']].values\n",
    "        start_end_distance = np.linalg.norm(end_position - start_position)\n",
    "\n",
    "        # Stocker les résultats dans un dictionnaire\n",
    "        metrics.append({\n",
    "            'experiment': experiment,\n",
    "            'particle': particle,\n",
    "            'displacement_sum [um]': displacement_sum,\n",
    "            'mean_vit_inst [um/min]': mean_vit_inst,\n",
    "            'median_vit_inst [um/min]': median_vit_inst,\n",
    "            'start_end_distance [um]': start_end_distance\n",
    "        })\n",
    "\n",
    "    # Conversion en DataFrame\n",
    "    metrics_df = pd.DataFrame(metrics)\n",
    "    return metrics_df\n",
    "\n",
    "# Appel de la fonction pour obtenir le DataFrame consolidé\n",
    "metrics_df = calculate_metrics(DATA)\n",
    "\n",
    "# Affichage pour vérification\n",
    "metrics_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_all_metrics(DATA):\n",
    "    \"\"\"\n",
    "    Calcule diverses métriques pour chaque expérience, y compris la vitesse moyenne\n",
    "    des particules sup et inf.\n",
    "    \"\"\"\n",
    "    # Calculer le temps d'incubation et le nombre de particules (exp_hours)\n",
    "    exp_hours = (\n",
    "        DATA.groupby('experiment')\n",
    "        .agg({\n",
    "            'time to incubation (hours)': 'first',\n",
    "            'particle': 'nunique'\n",
    "        })\n",
    "        .reset_index()\n",
    "        .rename(columns={'particle': 'number_of_particles'})\n",
    "    )\n",
    "\n",
    "    # Compter les particules is_inf et is_sup\n",
    "    particle_counts = (\n",
    "        DATA.groupby(['experiment', 'particle'])\n",
    "        .agg(\n",
    "            is_inf=('is_inf', 'max'),\n",
    "            is_sup=('is_sup', 'max')\n",
    "        )\n",
    "        .reset_index()\n",
    "        .groupby('experiment')\n",
    "        .agg(\n",
    "            number_of_inf=('is_inf', 'sum'),\n",
    "            number_of_sup=('is_sup', 'sum')\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "    particle_counts['proportion_inf'] = particle_counts['number_of_inf'] / (\n",
    "        particle_counts['number_of_inf'] + particle_counts['number_of_sup']\n",
    "    )\n",
    "\n",
    "    # Calculer les métriques issues de metrics_df\n",
    "    metrics_df = DATA.groupby(['experiment', 'particle']).apply(\n",
    "        lambda group: {\n",
    "            'displacement_sum [um]': group.head(200)['displacement [pix]'].sum(),\n",
    "            'start_end_distance [um]': np.linalg.norm(\n",
    "                group.head(200).iloc[0][['x', 'y']].values -\n",
    "                group.head(200).iloc[-1][['x', 'y']].values\n",
    "            ),\n",
    "            'mean_vit_inst [um/min]': group['VitInst [um/min]'].median()\n",
    "        }\n",
    "    ).apply(pd.Series).reset_index()\n",
    "\n",
    "    # Ajouter la vitesse mediane pour les particules sup et inf\n",
    "    sup_inf_speeds = DATA.groupby(['experiment', 'particle']).agg(\n",
    "        is_sup=('is_sup', 'max'),\n",
    "        is_inf=('is_inf', 'max'),\n",
    "        mean_vit_inst=('VitInst [um/min]', 'median')\n",
    "    ).reset_index()\n",
    "\n",
    "    # Calculer la vitesse mediane des particules sup et inf par expérience\n",
    "    sup_inf_speeds_agg = sup_inf_speeds.groupby('experiment').agg(\n",
    "        mean_speed_sup=('mean_vit_inst', lambda x: x[sup_inf_speeds['is_sup'] == 1].median() if (sup_inf_speeds['is_sup'] == 1).any() else 0),\n",
    "        mean_speed_inf=('mean_vit_inst', lambda x: x[sup_inf_speeds['is_inf'] == 1].median() if (sup_inf_speeds['is_inf'] == 1).any() else 0)\n",
    "    ).reset_index()\n",
    "\n",
    "    # Calculer taille et particules par champ\n",
    "    spatial_metrics = DATA.groupby('experiment').apply(lambda group: {\n",
    "        'taille': math.ceil(group['y'].max() / 2048) * math.ceil(group['x'].max() / 2048),\n",
    "        'nombre_part_par_champs': group['particle'].nunique() /\n",
    "        (math.ceil(group['y'].max() / 2048) * math.ceil(group['x'].max() / 2048)) if (\n",
    "            math.ceil(group['y'].max() / 2048) * math.ceil(group['x'].max() / 2048)) > 0 else 0\n",
    "    }).apply(pd.Series).reset_index()\n",
    "\n",
    "    # Calculer d'autres métriques\n",
    "    result_metrics = metrics_df.groupby('experiment').agg(\n",
    "        displacement_sum_mean=('displacement_sum [um]', 'median'),\n",
    "        start_end_distance_mean=('start_end_distance [um]', 'median'),\n",
    "        mean_speed=('mean_vit_inst [um/min]', 'median')\n",
    "    ).reset_index()\n",
    "\n",
    "    # Fusionner toutes les métriques\n",
    "    consolidated_metrics = (\n",
    "        exp_hours\n",
    "        .merge(particle_counts, on='experiment', how='outer')\n",
    "        .merge(result_metrics, on='experiment', how='outer')\n",
    "        .merge(sup_inf_speeds_agg, on='experiment', how='outer')\n",
    "        .merge(spatial_metrics, on='experiment', how='outer')\n",
    "    )\n",
    "\n",
    "    # Renommer les colonnes après la fusion\n",
    "    consolidated_metrics = consolidated_metrics.rename(columns={\n",
    "        'displacement_sum_mean': 'displacement_sum_mean [um]',\n",
    "        'start_end_distance_mean': 'start_end_distance_mean [um]',\n",
    "        'mean_speed': 'mean_speed [um/min]',\n",
    "        'mean_speed_sup': 'mean_speed_sup [um/min]',\n",
    "        'mean_speed_inf': 'mean_speed_inf [um/min]'\n",
    "    })\n",
    "\n",
    "    return consolidated_metrics\n",
    "\n",
    "# Appel de la fonction\n",
    "all_metrics_df = calculate_all_metrics(DATA)\n",
    "\n",
    "# Affichage pour vérification\n",
    "all_metrics_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DATA[DATA['is_inf']==True]['particle'].nunique())\n",
    "print(DATA['particle'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Displacement\n",
    "importlib.reload(lib)\n",
    "lib.plot_displacement(\n",
    "    DATA,\n",
    "    start_end=metrics_df[['particle', 'start_end_distance [um]']],\n",
    "    alpha=0.1,\n",
    "    linewidth=0.3,\n",
    "    ylim=[0, 750],\n",
    "    xlim=[0, DATA['time (min)'].max()],\n",
    "    save=True,\n",
    "    pathway_saving=path_save_pic,\n",
    "    name='displacement start-end all',\n",
    "    img_type=IMG_TYPE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Histograms of Start-End Distances\n",
    "def plot_histograms(start_end, PART_COEF_SUP, PART_COEF_INF, IMG_TYPE):\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(5, 10), sharex=True, sharey=True)\n",
    "    axes[0].hist(metrics_df['start_end_distance [um]'], bins=250, color='royalblue', alpha=0.7, density=True)\n",
    "    axes[0].set_xlim([0, 400])\n",
    "    axes[0].set_ylim([0, 0.03])\n",
    "    axes[0].set_title('Start-End Distances - All Particles', fontsize=16)\n",
    "    axes[0].set_ylabel('Density', fontsize=14)\n",
    "    axes[0].grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "    axes[1].hist(metrics_df[metrics_df['particle'].isin(PART_COEF_SUP)][['start_end_distance [um]']], bins=250, color='blue', alpha=0.7, density=True)\n",
    "    axes[1].set_xlim([0, 400])\n",
    "    axes[1].set_title('Start-End Distances - High Coefficient Particles', fontsize=16)\n",
    "    axes[1].set_ylabel('Density', fontsize=14)\n",
    "    axes[1].grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "    axes[2].hist(metrics_df[metrics_df['particle'].isin(PART_COEF_INF)][['start_end_distance [um]']], bins=250, color='red', alpha=0.7, density=True)\n",
    "    axes[2].set_xlim([0, 400])\n",
    "    axes[2].set_title('Start-End Distances - Low Coefficient Particles', fontsize=16)\n",
    "    axes[2].set_xlabel('Distance (μm)', fontsize=14)\n",
    "    axes[2].set_ylabel('Density', fontsize=14)\n",
    "    axes[2].grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(path_save_pic, f\"Nstart_end inf supp and all.{IMG_TYPE}\"), format=IMG_TYPE)\n",
    "    plt.show()\n",
    "\n",
    "plot_histograms(start_end, PART_COEF_SUP, PART_COEF_INF, IMG_TYPE=IMG_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Results in Function of Number of Particles per Field\n",
    "def plot_results_vs_particles(all_metrics_df, path_save_pic, CONDITION_simple, img_type='svg'):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 6))\n",
    "    ax1, ax2 = axes.flatten()\n",
    "    ax1.scatter(all_metrics_df['nombre_part_par_champs'], all_metrics_df['displacement_sum_mean [um]'], marker='+', color='orange')\n",
    "    ax1.set_title('Average distance traveled vs. Number of particles per field')\n",
    "    ax1.set_xlabel('Number of particles per field')\n",
    "    ax1.set_ylabel('Average distance traveled')\n",
    "    ax2.scatter(all_metrics_df['nombre_part_par_champs'], all_metrics_df['mean_speed [um/min]'], marker='+', color='blue')\n",
    "    ax2.set_title('Mean Speed vs. Number of particles per field')\n",
    "    ax2.set_xlabel('Number of particles per field')\n",
    "    ax2.set_ylabel('Mean Speed [μm/min]')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    fig.savefig(os.path.join(path_save_pic, f\"Results_vs_particles_{CONDITION_simple}.{img_type}\"), format=img_type)\n",
    "\n",
    "plot_results_vs_particles(all_metrics_df, path_save_pic, CONDITION_simple, img_type=IMG_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Angle Changes between Directions\n",
    "def compute_angle_changes(DATA, PART_COEF_SUP, PART_COEF_INF):\n",
    "    def angle_between_directions(row):\n",
    "        dx1, dy1 = row['dir_x'], row['dir_y']\n",
    "        dx2, dy2 = row['dir_x_next'], row['dir_y_next']\n",
    "        angle_initial = np.arctan2(dy1, dx1)\n",
    "        angle_final = np.arctan2(dy2, dx2)\n",
    "        angle_change = angle_final - angle_initial\n",
    "        angle_change = (angle_change + np.pi) % (2 * np.pi) - np.pi\n",
    "        return np.degrees(angle_change)\n",
    "\n",
    "    df_sup = DATA[DATA['particle'].isin(PART_COEF_SUP)].copy()\n",
    "    df_sup.sort_values(by=['particle', 'frame'], inplace=True)\n",
    "    df_sup['dir_x'] = df_sup.groupby('particle')['x'].diff().fillna(0)\n",
    "    df_sup['dir_y'] = df_sup.groupby('particle')['y'].diff().fillna(0)\n",
    "    df_sup['dir_x_next'] = df_sup.groupby('particle')['dir_x'].shift(-1)\n",
    "    df_sup['dir_y_next'] = df_sup.groupby('particle')['dir_y'].shift(-1)\n",
    "    df_sup['angle_change'] = df_sup.apply(angle_between_directions, axis=1)\n",
    "    df_sup.dropna(subset=['dir_x_next', 'dir_y_next'], inplace=True)\n",
    "\n",
    "    df_inf = DATA[DATA['particle'].isin(PART_COEF_INF)].copy()\n",
    "    df_inf.sort_values(by=['particle', 'frame'], inplace=True)\n",
    "    df_inf['dir_x'] = df_inf.groupby('particle')['x'].diff().fillna(0)\n",
    "    df_inf['dir_y'] = df_inf.groupby('particle')['y'].diff().fillna(0)\n",
    "    df_inf['dir_x_next'] = df_inf.groupby('particle')['dir_x'].shift(-1)\n",
    "    df_inf['dir_y_next'] = df_inf.groupby('particle')['dir_y'].shift(-1)\n",
    "    df_inf['angle_change'] = df_inf.apply(angle_between_directions, axis=1)\n",
    "    df_inf.dropna(subset=['dir_x_next', 'dir_y_next'], inplace=True)\n",
    "\n",
    "    return df_sup, df_inf\n",
    "\n",
    "df_sup, df_inf = compute_angle_changes(DATA, PART_COEF_SUP, PART_COEF_INF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms of angle changes\n",
    "def plot_angle_histograms(df_all, df_sup, df_inf, img_type='svg'):\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(5, 10), sharex=True)\n",
    "    axes[0].hist(df_all['angle_change'], bins=1000, alpha=0.5, color='green')\n",
    "    axes[0].set_title('Angle Changes - All Particles')\n",
    "    axes[0].set_xlabel('Angle (degrees)')\n",
    "    axes[0].set_ylabel('Density')\n",
    "\n",
    "    axes[1].hist(df_sup['angle_change'], bins=1000, color='blue', alpha=0.5)\n",
    "    axes[1].set_title('Angle Changes - High Coefficient Particles')\n",
    "    axes[1].set_xlabel('Angle (degrees)')\n",
    "    axes[1].set_ylabel('Density')\n",
    "\n",
    "    axes[2].hist(df_inf['angle_change'], bins=1000, color='red', alpha=0.5)\n",
    "    axes[2].set_title('Angle Changes - Low Coefficient Particles')\n",
    "    axes[2].set_xlabel('Angle (degrees)')\n",
    "    axes[2].set_ylabel('Density')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    fig.savefig(os.path.join(path_save_pic, f'Angle_Changes_Histograms.{img_type}'), format=img_type)\n",
    "\n",
    "df_all = pd.concat([df_sup, df_inf])\n",
    "plot_angle_histograms(df_all, df_sup, df_inf, img_type=IMG_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mean_vit_histograms_comparison(metrics_df, part_coef_sup, part_coef_inf, alpha=0.5, path_save_pic=None, img_type='png'):\n",
    "    \"\"\"\n",
    "    Trace sur le même graphique les histogrammes des mean_vit_inst [um/min]\n",
    "    pour les particules avec coefficients supérieurs et inférieurs.\n",
    "\n",
    "    Parameters:\n",
    "    - metrics_df (DataFrame): DataFrame contenant les données, avec une colonne 'mean_vit_inst [um/min]'.\n",
    "    - part_coef_sup (list): Liste des particules avec coefficients supérieurs.\n",
    "    - part_coef_inf (list): Liste des particules avec coefficients inférieurs.\n",
    "    - path_save_pic (str, optional): Chemin pour sauvegarder l'image. Si None, l'image ne sera pas sauvegardée.\n",
    "    - img_type (str, optional): Format de l'image pour la sauvegarde (par défaut 'png').\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Vérification de la présence de la colonne nécessaire\n",
    "    if 'mean_vit_inst [um/min]' not in metrics_df.columns:\n",
    "        raise ValueError(\"La colonne 'mean_vit_inst [um/min]' est absente de metrics_df.\")\n",
    "    \n",
    "    # Filtrer les données\n",
    "    df_sup = metrics_df[metrics_df['particle'].isin(part_coef_sup)]\n",
    "    df_inf = metrics_df[metrics_df['particle'].isin(part_coef_inf)]\n",
    "    \n",
    "    median_value_sup = df_sup['median_vit_inst [um/min]'].median()\n",
    "    median_value_inf = df_inf['median_vit_inst [um/min]'].median()\n",
    "\n",
    "    # Préparer le graphique\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.hist(df_sup['median_vit_inst [um/min]'], bins=100, color='blue', alpha=alpha, label=\"hight slope\", density=True)\n",
    "    plt.hist(df_inf['median_vit_inst [um/min]'], bins=100, color='red', alpha=alpha, label=\"low slope\", density=True)\n",
    "\n",
    "     # Ajouter une barre verticale à la médiane\n",
    "    plt.axvline(median_value_inf, color='red', linestyle='--', linewidth=2, label=f'Median: {median_value_inf:.2f} μm/min')\n",
    "    plt.axvline(median_value_sup, color='blue', linestyle='--', linewidth=2, label=f'Median: {median_value_sup:.2f} μm/min')\n",
    "\n",
    "\n",
    "    # Ajouter des titres et des légendes\n",
    "    plt.title(\"Median speed [μm/min]\", fontsize=14)\n",
    "    plt.xlabel(\"Median Instantaneous Velocity [μm/min]\", fontsize=12)\n",
    "    plt.ylabel(\"Density\", fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.legend(fontsize=12)\n",
    "    \n",
    "    # Sauvegarde de l'image si un chemin est spécifié\n",
    "    if path_save_pic:\n",
    "        save_path = os.path.join(path_save_pic, f'Median_Velocity_Histograms_Comparison.{img_type}')\n",
    "        plt.savefig(save_path, format=img_type)\n",
    "        print(f\"Plot sauvegardé à : {save_path}\")\n",
    "    \n",
    "    # Afficher le graphique\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mean_vit_histograms_comparison(metrics_df, PART_COEF_SUP, PART_COEF_INF, path_save_pic=path_save_pic, img_type=IMG_TYPE, alpha=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_vit_inst_histograms_comparison(metrics_df, part_coef_sup, part_coef_inf, alpha=0.5, path_save_pic=None, img_type='png'):\n",
    "    \"\"\"\n",
    "    Trace sur le même graphique les histogrammes des mean_vit_inst [um/min]\n",
    "    pour les particules avec coefficients supérieurs et inférieurs.\n",
    "\n",
    "    Parameters:\n",
    "    - metrics_df (DataFrame): DataFrame contenant les données, avec une colonne 'mean_vit_inst [um/min]'.\n",
    "    - part_coef_sup (list): Liste des particules avec coefficients supérieurs.\n",
    "    - part_coef_inf (list): Liste des particules avec coefficients inférieurs.\n",
    "    - path_save_pic (str, optional): Chemin pour sauvegarder l'image. Si None, l'image ne sera pas sauvegardée.\n",
    "    - img_type (str, optional): Format de l'image pour la sauvegarde (par défaut 'png').\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Vérification de la présence de la colonne nécessaire\n",
    "    if 'mean_vit_inst [um/min]' not in metrics_df.columns:\n",
    "        raise ValueError(\"La colonne 'mean_vit_inst [um/min]' est absente de metrics_df.\")\n",
    "    \n",
    "    # Filtrer les données\n",
    "    df_sup = DATA[DATA['particle'].isin(part_coef_sup)]\n",
    "    df_inf = DATA[DATA['particle'].isin(part_coef_inf)]\n",
    "    \n",
    "    instant_value_sup = df_sup['VitInst [um/min]']\n",
    "    instant_value_inf = df_inf['VitInst [um/min]']\n",
    "\n",
    "    # Préparer le graphique\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.hist(df_sup['VitInst [um/min]'], bins=100, color='blue', alpha=alpha, label=\"hight slope\", density=True)\n",
    "    plt.hist(df_inf['VitInst [um/min]'], bins=100, color='red', alpha=alpha, label=\"low slope\", density=True)\n",
    "\n",
    "    #  # Ajouter une barre verticale à la médiane\n",
    "    # plt.axvline(median_value_inf, color='red', linestyle='--', linewidth=2, label=f'Median: {median_value_inf:.2f} μm/min')\n",
    "    # plt.axvline(median_value_sup, color='blue', linestyle='--', linewidth=2, label=f'Median: {median_value_sup:.2f} μm/min')\n",
    "\n",
    "\n",
    "    # Ajouter des titres et des légendes\n",
    "    plt.title(\"Median speed [μm/min]\", fontsize=14)\n",
    "    plt.xlabel(\"Median Instantaneous Velocity [μm/min]\", fontsize=12)\n",
    "    plt.ylabel(\"Density\", fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.legend(fontsize=12)\n",
    "    \n",
    "    # Sauvegarde de l'image si un chemin est spécifié\n",
    "    if path_save_pic:\n",
    "        save_path = os.path.join(path_save_pic, f'Inst_Velocity_Histograms_Comparison.{img_type}')\n",
    "        plt.savefig(save_path, format=img_type)\n",
    "        print(f\"Plot sauvegardé à : {save_path}\")\n",
    "    \n",
    "    # Afficher le graphique\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_vit_inst_histograms_comparison(metrics_df, PART_COEF_SUP, PART_COEF_INF, path_save_pic=path_save_pic, img_type=IMG_TYPE, alpha=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préparer le graphique\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.hist(DATA['VitInst [um/min]'], bins=500, color='limegreen', alpha=0.8, label=\"all speed\", density=True)\n",
    "\n",
    "# Ajouter une barre verticale à la médiane\n",
    "# plt.axvline(median_value, color='red', linestyle='--', linewidth=2, label=f'Median: {median_value:.2f} μm/min')\n",
    "\n",
    "# Ajouter des titres et des légendes\n",
    "plt.title(\"VitInst[μm/min]\", fontsize=14)\n",
    "plt.xlabel(\"Instantaneous Velocity [μm/min]\", fontsize=12)\n",
    "plt.ylabel(\"Density\", fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.legend(fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mean_vit_histograms_comparison(metrics_df, path_save_pic=None, img_type='png', alpha=0.7):\n",
    "    \"\"\"\n",
    "    Trace sur le même graphique les histogrammes des mean_vit_inst [um/min]\n",
    "    pour les particules avec coefficients supérieurs et inférieurs.\n",
    "\n",
    "    Parameters:\n",
    "    - metrics_df (DataFrame): DataFrame contenant les données, avec une colonne 'mean_vit_inst [um/min]'.\n",
    "    - path_save_pic (str, optional): Chemin pour sauvegarder l'image. Si None, l'image ne sera pas sauvegardée.\n",
    "    - img_type (str, optional): Format de l'image pour la sauvegarde (par défaut 'png').\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Vérification de la présence de la colonne nécessaire\n",
    "    if 'median_vit_inst [um/min]' not in metrics_df.columns:\n",
    "        raise ValueError(\"La colonne 'median_vit_inst [um/min]' est absente de metrics_df.\")\n",
    "    \n",
    "    # Calcul de la médiane\n",
    "    median_value = metrics_df['median_vit_inst [um/min]'].median()\n",
    "    print(f\"Median value: {median_value}\")\n",
    "\n",
    "    # Préparer le graphique\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.hist(metrics_df['median_vit_inst [um/min]'], bins=100, color='limegreen', alpha=alpha, label=\"all speed\", density=True)\n",
    "\n",
    "    # Ajouter une barre verticale à la médiane\n",
    "    plt.axvline(median_value, color='red', linestyle='--', linewidth=2, label=f'Median: {median_value:.2f} μm/min')\n",
    "\n",
    "    # Ajouter des titres et des légendes\n",
    "    plt.title(\"Median speed [μm/min]\", fontsize=14)\n",
    "    plt.xlabel(\"Median Instantaneous Velocity [μm/min]\", fontsize=12)\n",
    "    plt.ylabel(\"Density\", fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.legend(fontsize=12)\n",
    "    \n",
    "    # Sauvegarde de l'image si un chemin est spécifié\n",
    "    if path_save_pic:\n",
    "        save_path = os.path.join(path_save_pic, f'Median_Velocity_Histograms_all.{img_type}')\n",
    "        plt.savefig(save_path, format=img_type)\n",
    "        print(f\"Plot sauvegardé à : {save_path}\")\n",
    "    \n",
    "    # Afficher le graphique\n",
    "    # Sans tight_layout (les titres peuvent se chevaucher)\n",
    "    plt.tight_layout()  # Ajuste automatiquement l'espacement\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mean_vit_histograms_comparison(metrics_df, path_save_pic=path_save_pic, img_type=IMG_TYPE, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_velocity_histograms(all_metrics_df, metrics_df, path_save_pic, img_type):\n",
    "    \"\"\"\n",
    "    Plot histograms of median instantaneous velocities for each experiment, sorted by incubation time.\n",
    "\n",
    "    Parameters:\n",
    "    - all_metrics_df (DataFrame): Aggregated metrics per experiment.\n",
    "    - metrics_df (DataFrame): Metrics for each particle and experiment.\n",
    "    - path_save_pic (str): Path where the plot will be saved.\n",
    "    - img_type (str): Image format for saving the plot.\n",
    "    \"\"\"\n",
    "    # Trier les expériences par temps d'incubation\n",
    "    exp_hours = all_metrics_df[['experiment', 'time to incubation (hours)']].sort_values(by='time to incubation (hours)')\n",
    "    experiments = exp_hours['experiment'].tolist()\n",
    "    incubation_times = exp_hours['time to incubation (hours)'].tolist()\n",
    "\n",
    "    # Déterminer les bornes de l'axe X\n",
    "    global_min = metrics_df['median_vit_inst [um/min]'].min()\n",
    "    global_max = metrics_df['median_vit_inst [um/min]'].max()\n",
    "    delta = (global_max - global_min) * 0.1  # Ajouter 10% de marge pour l'affichage\n",
    "    global_min -= delta\n",
    "    global_max += delta\n",
    "\n",
    "    # Liste pour stocker les médianes des vitesses moyennes instantanées\n",
    "    median_values_list = []\n",
    "\n",
    "    # Nombre d'expériences à afficher\n",
    "    n_experiments = len(experiments)\n",
    "    fig, axes = plt.subplots(nrows=n_experiments, ncols=1, figsize=(10, 2 * n_experiments), sharex=True, sharey=True)\n",
    "\n",
    "    if n_experiments == 1:\n",
    "        axes = [axes]  # S'assurer que 'axes' est une liste si un seul subplot\n",
    "\n",
    "    # Palette de couleurs\n",
    "    palette = colormaps['tab20']\n",
    "    colors = [palette(i / n_experiments) for i in range(n_experiments)]\n",
    "\n",
    "    # Boucle sur chaque expérience\n",
    "    for idx, ax in enumerate(axes):\n",
    "        exp = experiments[idx]\n",
    "        hour = incubation_times[idx]\n",
    "\n",
    "        # Filtrer les données pour cette expérience\n",
    "        data_exp = metrics_df[metrics_df['experiment'] == exp]\n",
    "        num_particles = len(data_exp)  # Nombre de particules dans cette expérience\n",
    "\n",
    "        # Calculer la médiane des vitesses moyennes instantanées pour cette expérience\n",
    "        median_value = data_exp['median_vit_inst [um/min]'].median()\n",
    "        median_values_list.append(median_value)  # Stocker la médiane\n",
    "\n",
    "        # Tracer l'histogramme\n",
    "        ax.hist(data_exp['median_vit_inst [um/min]'], bins=30, alpha=0.3, range=(global_min, global_max), color=colors[idx])\n",
    "\n",
    "        # Ajouter titre et labels\n",
    "        ax.set_title(f'Vit. Inst. Mediane [μm/min] - {exp} (Incubation: {hour}h)')\n",
    "        ax.set_xlabel('VitInst [μm/min]')\n",
    "        ax.set_ylabel('Nombre de particules')\n",
    "\n",
    "        # Ajouter la médiane avec une ligne verticale rouge\n",
    "        ax.axvline(median_value, color='red', linestyle='dashed', linewidth=1)\n",
    "        ax.text(\n",
    "            median_value + 0.05 * (global_max - global_min), \n",
    "            # ax.get_ylim()[1] * 0.95,\n",
    "            75,\n",
    "            f'Médiane: {median_value:.2f}',\n",
    "            color='red',\n",
    "            ha='left'\n",
    "        )\n",
    "\n",
    "        # Ajouter le nombre total de particules sur le graphique\n",
    "        ax.text(\n",
    "            global_max - 0.1 * (global_max - global_min),  # Position X\n",
    "            # ax.get_ylim()[1] * 0.85,  # Position Y (haut du graphe)\n",
    "            75,\n",
    "            f'Nb Particules: {num_particles}',\n",
    "            color='black',\n",
    "            ha='right',\n",
    "            fontsize=10,\n",
    "            bbox=dict(facecolor='white', alpha=0.5, edgecolor='black')\n",
    "        )\n",
    "\n",
    "        # Ajuster les limites de l'axe X\n",
    "        ax.set_xlim(global_min, global_max)\n",
    "\n",
    "    # Calculer la moyenne et la médiane des médianes enregistrées\n",
    "    overall_median_of_medians = np.median(median_values_list)\n",
    "    overall_mean_of_medians = np.mean(median_values_list)\n",
    "\n",
    "    # Afficher les résultats finaux\n",
    "    print(f\"Médiane globale des médianes : {overall_median_of_medians:.2f} μm/min\")\n",
    "    print(f\"Moyenne globale des médianes : {overall_mean_of_medians:.2f} μm/min\")\n",
    "\n",
    "    # Ajuster l'affichage et sauvegarder la figure\n",
    "    plt.tight_layout()\n",
    "    fig_path = os.path.join(path_save_pic, f'Velocity_Histograms.{img_type}')\n",
    "    fig.savefig(fig_path, format=img_type)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Plot saved to {fig_path}\")\n",
    "\n",
    "    return overall_median_of_medians, overall_mean_of_medians  # Retourner ces valeurs si besoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_velocity_histograms(all_mean_vitinst, medians, exp_hours, path_save_pic, IMG_TYPE)\n",
    "plot_velocity_histograms(all_metrics_df, metrics_df, path_save_pic, IMG_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "<center><span style=\"color: skyblue; font-size: 20px\"> Calcul de l'autocorrelation de la vitesse </span></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def velocity_autocorrelation_per_particle(data, max_lag, fps):\n",
    "    \"\"\"\n",
    "    Calcule l'autocorrélation de la vitesse pour chaque particule individuellement.\n",
    "\n",
    "    Paramètres:\n",
    "    ----------\n",
    "    - data : pd.DataFrame\n",
    "        DataFrame contenant les trajectoires, avec les colonnes :\n",
    "        ['experiment', 'particle', 'frame', 'dx [pix]', 'dy [pix]']\n",
    "    - max_lag : int\n",
    "        Nombre maximum d'intervalle de temps (tau) pour l'autocorrélation.\n",
    "    - fps : float\n",
    "        Nombre d'images par seconde, pour convertir le temps en secondes.\n",
    "\n",
    "    Retourne:\n",
    "    ----------\n",
    "    - autocorr_df : pd.DataFrame\n",
    "        DataFrame contenant l'autocorrélation moyenne et individuelle par particule.\n",
    "    \"\"\"\n",
    "\n",
    "    # Vérifier que les colonnes nécessaires existent\n",
    "    required_columns = {'experiment', 'particle', 'frame', 'dx [pix]', 'dy [pix]'}\n",
    "    if not required_columns.issubset(data.columns):\n",
    "        raise KeyError(f\"🚨 Erreur : Les colonnes requises {required_columns} ne sont pas toutes présentes dans DATA.\")\n",
    "\n",
    "    # Trier les données\n",
    "    data = data.sort_values(by=['experiment', 'particle', 'frame'])\n",
    "\n",
    "    # Calculer les composantes de vitesse vx et vy\n",
    "    data['vx'] = data['dx [pix]'] / data.groupby(['experiment', 'particle'])['frame'].diff()\n",
    "    data['vy'] = data['dy [pix]'] / data.groupby(['experiment', 'particle'])['frame'].diff()\n",
    "\n",
    "    # Remplir les NaN initiaux (première frame de chaque particule)\n",
    "    data[['vx', 'vy']] = data[['vx', 'vy']].fillna(0)\n",
    "\n",
    "    # Dictionnaire pour stocker les résultats\n",
    "    autocorr_dict = {'lag': []}\n",
    "    particle_corrs = {}\n",
    "\n",
    "    # Parcourir chaque particule et calculer son autocorrélation individuelle\n",
    "    for particle, group in data.groupby(['experiment', 'particle']):\n",
    "        group = group.sort_values(by='frame')\n",
    "\n",
    "        # Vérifier qu'il y a assez de points pour calculer l'autocorrélation\n",
    "        if len(group) < max_lag:\n",
    "            continue\n",
    "\n",
    "        v_t = np.sqrt(group['vx']**2 + group['vy']**2).values\n",
    "\n",
    "        autocorr_values = []\n",
    "        for tau in range(1, max_lag + 1):\n",
    "            v_tau = np.roll(v_t, -tau)  # Décaler les vitesses de tau frames\n",
    "            valid_idx = np.arange(len(v_t)) < (len(v_t) - tau)  # Éviter les valeurs hors limites\n",
    "\n",
    "            # Calculer l'autocorrélation\n",
    "            corr = np.mean(v_t[valid_idx] * v_tau[valid_idx]) / np.mean(v_t[valid_idx]**2)\n",
    "            autocorr_values.append(corr)\n",
    "\n",
    "        particle_corrs[particle] = autocorr_values\n",
    "\n",
    "    # Transformer en DataFrame\n",
    "    autocorr_df = pd.DataFrame.from_dict(particle_corrs, orient='index', columns=[f'lag_{tau}' for tau in range(1, max_lag + 1)])\n",
    "    autocorr_df.index.name = 'particle'\n",
    "\n",
    "    # Calculer la moyenne des autocorrélations par lag\n",
    "    autocorr_df.loc['mean'] = autocorr_df.mean()\n",
    "\n",
    "    return autocorr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramètres\n",
    "max_lag = 30\n",
    "fps = FPS  # images par seconde\n",
    "lags = np.arange(1, max_lag + 1) / fps  # en secondes\n",
    "\n",
    "# Filtrer les données selon les listes d’IDs\n",
    "data_inf = DATA[DATA['particle'].isin(PART_COEF_INF)]\n",
    "data_sup = DATA[DATA['particle'].isin(PART_COEF_SUP)]\n",
    "\n",
    "# --- calculs précédents ---\n",
    "ac_inf = velocity_autocorrelation_per_particle(data_inf, max_lag=max_lag, fps=fps)\n",
    "ac_sup = velocity_autocorrelation_per_particle(data_sup, max_lag=max_lag, fps=fps)\n",
    "\n",
    "# Extraction de la moyenne\n",
    "mean_inf = ac_inf.loc['mean'].values\n",
    "mean_sup = ac_sup.loc['mean'].values\n",
    "\n",
    "# Normalisation : on ramène chaque courbe à 1 au premier lag\n",
    "norm_inf = mean_inf / mean_inf[0]\n",
    "norm_sup = mean_sup / mean_sup[0]\n",
    "\n",
    "# Tracé\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(lags, norm_inf, marker='o', label='Inférieure (normalisée)')\n",
    "plt.plot(lags, norm_sup, marker='s', label='Supérieure (normalisée)')\n",
    "plt.xlabel(\"Décalage temporel (s)\")\n",
    "plt.ylabel(\"Autocorrélation de la vitesse (normalisée)\")\n",
    "plt.title(\"Autocorrélation normalisée par sous-population\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def velocity_autocorrelation_by_experiment_and_global(data, max_lag, fps):\n",
    "    \"\"\"\n",
    "    Calcule l'autocorrélation de la vitesse par particule, puis :\n",
    "      1) Moyenne par expérience\n",
    "      2) Moyenne globale (sur toutes les particules de toutes les expériences).\n",
    "    \n",
    "    Retourne un DataFrame dont :\n",
    "      - L'index = décalages en secondes (1/fps, 2/fps, ..., max_lag/fps).\n",
    "      - Les colonnes = chaque expérience + la colonne 'All' (moyenne globale).\n",
    "    \n",
    "    Paramètres\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        DataFrame contenant les trajectoires, avec colonnes :\n",
    "        ['experiment', 'particle', 'frame', 'dx [pix]', 'dy [pix]']\n",
    "    max_lag : int\n",
    "        Nombre maximum d'intervalle de temps (tau) pour l'autocorrélation.\n",
    "    fps : float\n",
    "        Nombre d'images par seconde, pour convertir frames -> secondes.\n",
    "    \n",
    "    Retourne\n",
    "    --------\n",
    "    autocorr_df : pd.DataFrame\n",
    "        * index : lags (s)\n",
    "        * colonnes : expériences + 'All'\n",
    "    \"\"\"\n",
    "    # Vérifier colonnes\n",
    "    required_cols = {'experiment', 'particle', 'frame', 'dx [pix]', 'dy [pix]'}\n",
    "    if not required_cols.issubset(data.columns):\n",
    "        raise KeyError(\n",
    "            f\"🚨 Les colonnes requises {required_cols} ne sont pas toutes présentes dans 'data'.\"\n",
    "        )\n",
    "    \n",
    "    # Trier la DataFrame\n",
    "    data = data.sort_values(by=['experiment', 'particle', 'frame'])\n",
    "    \n",
    "    # Calcul des composantes de vitesse vx, vy\n",
    "    data['delta_frame'] = data.groupby(['experiment', 'particle'])['frame'].diff()\n",
    "    data['vx'] = data['dx [pix]'] / data['delta_frame']\n",
    "    data['vy'] = data['dy [pix]'] / data['delta_frame']\n",
    "    data[['vx', 'vy']] = data[['vx', 'vy']].fillna(0)\n",
    "    \n",
    "    # Stockage des autocorrélations\n",
    "    # experiment_autocorrs[exp_name] = [ [autocorr_part1], [autocorr_part2], ... ]\n",
    "    experiment_autocorrs = {}\n",
    "    # Liste globale de toutes les particules, toutes expériences confondues\n",
    "    all_autocorrs = []\n",
    "    \n",
    "    # Calcul de l'autocorrélation par particule\n",
    "    for (exp_name, particle_id), group in data.groupby(['experiment', 'particle']):\n",
    "        group = group.sort_values(by='frame')\n",
    "        \n",
    "        # Norme de la vitesse\n",
    "        v_t = np.sqrt(group['vx']**2 + group['vy']**2).values\n",
    "        \n",
    "        if len(v_t) < max_lag:\n",
    "            # Pas assez de points\n",
    "            continue\n",
    "        \n",
    "        # Calcul de l'autocorrélation pour tau = 1..max_lag\n",
    "        autocorr_values = []\n",
    "        for tau in range(1, max_lag + 1):\n",
    "            v_tau = np.roll(v_t, -tau)\n",
    "            valid_idx = np.arange(len(v_t)) < (len(v_t) - tau)\n",
    "            \n",
    "            numerator = np.mean(v_t[valid_idx] * v_tau[valid_idx])\n",
    "            denominator = np.mean(v_t[valid_idx]**2)\n",
    "            corr = numerator / denominator if denominator != 0 else np.nan\n",
    "            autocorr_values.append(corr)\n",
    "        \n",
    "        # Stocker dans experiment_autocorrs\n",
    "        if exp_name not in experiment_autocorrs:\n",
    "            experiment_autocorrs[exp_name] = []\n",
    "        experiment_autocorrs[exp_name].append(autocorr_values)\n",
    "        \n",
    "        # Stocker aussi dans la liste globale\n",
    "        all_autocorrs.append(autocorr_values)\n",
    "    \n",
    "    # Moyenne par expérience\n",
    "    experiment_mean_corr = {}\n",
    "    for exp_name, list_autocorrs in experiment_autocorrs.items():\n",
    "        arr = np.array(list_autocorrs)  # shape: (nb_particules, max_lag)\n",
    "        mean_by_lag = np.nanmean(arr, axis=0)\n",
    "        experiment_mean_corr[exp_name] = mean_by_lag\n",
    "    \n",
    "    # Moyenne globale (All) sur toutes les particules de TOUTES les expériences\n",
    "    if len(all_autocorrs) > 0:\n",
    "        arr_all = np.array(all_autocorrs)  # shape: (nb_total_particules, max_lag)\n",
    "        global_mean = np.nanmean(arr_all, axis=0)\n",
    "    else:\n",
    "        global_mean = np.full(shape=(max_lag,), fill_value=np.nan)\n",
    "    \n",
    "    # Construire le DataFrame final\n",
    "    lags_in_sec = np.arange(1, max_lag + 1) / fps\n",
    "    autocorr_df = pd.DataFrame(experiment_mean_corr, index=lags_in_sec)\n",
    "    autocorr_df.index.name = \"lag (s)\"\n",
    "    \n",
    "    # Ajouter la colonne 'All'\n",
    "    autocorr_df['All'] = global_mean\n",
    "    \n",
    "    return autocorr_df\n",
    "\n",
    "max_lag = 30\n",
    "fps = FPS  # par exemple\n",
    "\n",
    "autocorr_df = velocity_autocorrelation_by_experiment_and_global(DATA, max_lag, fps)\n",
    "\n",
    "# Création de la figure\n",
    "plt.figure()#figsize=(8, 5))\n",
    "\n",
    "# Tracé de \"All\" en bleu avec un style spécifique\n",
    "plt.plot(autocorr_df.index, autocorr_df[\"All\"], marker='o', linestyle='-', \n",
    "         label=\"All\", color=\"blue\", linewidth=2, alpha=1.0)\n",
    "\n",
    "# Tracé des expériences avec transparence et ligne fine\n",
    "for col in autocorr_df.columns:\n",
    "    if col != \"All\":  # On exclut \"All\"\n",
    "        plt.plot(autocorr_df.index, autocorr_df[col], marker='+', linestyle='-', \n",
    "                 alpha=0.5, linewidth=0.5, label=col)\n",
    "\n",
    "# Ajout de légende unique\n",
    "# plt.legend(title=\"Expériences et All\", loc=\"upper right\")\n",
    "\n",
    "# Axes et titre\n",
    "plt.xlabel(\"Lag (s)\")\n",
    "plt.ylabel(\"Autocorrélation\")\n",
    "plt.title(\"Autocorrélation de la vitesse, par expérience et globale\")\n",
    "plt.grid(True)\n",
    "\n",
    "# Affichage\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit exponnentiel de l'autocorrelation de la vitesse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><span style=\"color: skyblue; font-size: 20px\"> Mean speed vs time to incubation </span></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_mean_speed_with_sup_inf(all_metrics_df, path_save_pic, xlim: list = None):\n",
    "#     \"\"\"\n",
    "#     Plot mean speed, mean speed sup, and mean speed inf as a function of incubation time\n",
    "#     on the same graph.\n",
    "\n",
    "#     Parameters:\n",
    "#     - all_metrics_df (DataFrame): DataFrame containing aggregated metrics for each experiment.\n",
    "#                                   Must include columns: 'time to incubation (hours)',\n",
    "#                                   'mean_speed [um/min]', 'mean_speed_sup [um/min]', \n",
    "#                                   and 'mean_speed_inf [um/min]'.\n",
    "#     - path_save_pic (str): Path where the plot will be saved.\n",
    "#     - xlim (list, optional): Limit for the x-axis.\n",
    "\n",
    "#     Returns:\n",
    "#     None\n",
    "#     \"\"\"\n",
    "#     # Trier les données par temps d'incubation\n",
    "#     all_metrics_sorted = all_metrics_df.sort_values(by='time to incubation (hours)')\n",
    "\n",
    "#     # Préparer le tracé\n",
    "#     plt.figure(figsize=(10, 5))\n",
    "\n",
    "#     # Courbe pour la vitesse moyenne totale\n",
    "#     plt.plot(\n",
    "#         all_metrics_sorted['time to incubation (hours)'],\n",
    "#         all_metrics_sorted['mean_speed [um/min]'],\n",
    "#         linestyle='--', color='green', alpha=0.6,\n",
    "#         label='Mean Speed (Total)'\n",
    "#     )\n",
    "\n",
    "#     # Courbe pour les particules sup\n",
    "#     plt.plot(\n",
    "#         all_metrics_sorted['time to incubation (hours)'],\n",
    "#         all_metrics_sorted['mean_speed_sup [um/min]'],\n",
    "#         linestyle='--', color='blue', alpha=0.6,\n",
    "#         label='Mean Speed (Sup)'\n",
    "#     )\n",
    "\n",
    "#     # Courbe pour les particules inf\n",
    "#     plt.plot(\n",
    "#         all_metrics_sorted['time to incubation (hours)'],\n",
    "#         all_metrics_sorted['mean_speed_inf [um/min]'],\n",
    "#         linestyle='--', color='red', alpha=0.6,\n",
    "#         label='Mean Speed (Inf)'\n",
    "#     )\n",
    "\n",
    "#     # Ajouter des labels et des titres\n",
    "#     plt.title('Mean Speed vs. Time to Incubation (Total, Sup, Inf)', fontsize=14)\n",
    "#     plt.xlabel('Time to Incubation (hours)', fontsize=12)\n",
    "#     plt.ylabel('Mean Speed [μm/min]', fontsize=12)\n",
    "#     plt.ylim([0, 15])  # Limites pour l'axe des y\n",
    "#     if xlim:\n",
    "#         plt.xlim(xlim)  # Limites pour l'axe des x (optionnel)\n",
    "#     plt.grid(True, linestyle='--', alpha=0.6)\n",
    "#     plt.legend(fontsize=10)\n",
    "\n",
    "#     # Sauvegarder et afficher\n",
    "#     plt.tight_layout()\n",
    "#     fig_path = os.path.join(path_save_pic, 'Mean_Speed_vs_Time_Sup_Inf.png')\n",
    "#     plt.savefig(fig_path, format='png')\n",
    "#     plt.show()\n",
    "\n",
    "#     print(f\"Plot saved to {fig_path}\")\n",
    "\n",
    "# # Exemple d'appel de la fonction\n",
    "# plot_mean_speed_with_sup_inf(all_metrics_df, path_save_pic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_median_velocity_vs_time(all_metrics_df, path_save_pic, xlim: list = None):\n",
    "    \"\"\"\n",
    "    Plot median velocities as a function of incubation time using all_metrics_df.\n",
    "    Adds a trendline with a transparent error band to the data.\n",
    "\n",
    "    Parameters:\n",
    "    - all_metrics_df (DataFrame): DataFrame containing aggregated metrics for each experiment.\n",
    "    - path_save_pic (str): Path where the plot will be saved.\n",
    "    - xlim (list, optional): Limit for the x-axis.\n",
    "    \"\"\"\n",
    "    # Trier les données par temps d'incubation\n",
    "    all_metrics_sorted = all_metrics_df.sort_values(by='time to incubation (hours)')\n",
    "\n",
    "    # Extraire les données\n",
    "    x = all_metrics_sorted['time to incubation (hours)'].values\n",
    "    y = all_metrics_sorted['mean_speed [um/min]'].values\n",
    "\n",
    "    # Préparer le tracé\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.scatter(x, y, color='forestgreen', label='Médiane')  # Points\n",
    "    plt.plot(x, y, linestyle='--', color='limegreen', alpha=0.6)  # Ligne connectant les points\n",
    "\n",
    "    # Régression linéaire (y = ax + b)\n",
    "    slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
    "    trendline = slope * x + intercept\n",
    "\n",
    "    # Calcul de la zone d'erreur\n",
    "    error_band = 100*std_err * np.sqrt(1 / len(x) + (x - np.mean(x))**2 / np.sum((x - np.mean(x))**2))\n",
    "    # Ajouter la ligne de tendance\n",
    "    plt.plot(x, trendline, color='red', label=f\"Tendance (y = {slope:.2f}x + {intercept:.2f})\", linestyle='-', linewidth=1.5)\n",
    "\n",
    "    # Ajouter l'aire d'erreur\n",
    "    plt.fill_between(x, trendline - error_band, trendline + error_band, color='red', alpha=0.2, label=\"Marge d'erreur\")\n",
    "\n",
    "    # Ajouter des labels et des titres\n",
    "    plt.title('Mean Speed vs. Time to Incubation', fontsize=14)\n",
    "    plt.xlabel('Time to Incubation (hours)', fontsize=12)\n",
    "    plt.ylabel('Mean Speed [μm/min]', fontsize=12)\n",
    "    plt.ylim([0, 15])\n",
    "    if xlim:\n",
    "        plt.xlim(xlim)\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.legend(fontsize=10)\n",
    "\n",
    "    # Sauvegarder et afficher\n",
    "    plt.tight_layout()\n",
    "    fig_path = os.path.join(path_save_pic, 'Mean_Speed_vs_Time_with_Trendline_and_ErrorBand.png')\n",
    "    plt.savefig(fig_path, format='png')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Plot saved to {fig_path}\")\n",
    "\n",
    "#utilisation\n",
    "plot_median_velocity_vs_time(all_metrics_df, path_save_pic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_median_velocity_vs_time(all_metrics_df, path_save_pic, xlim: list = None):\n",
    "    \"\"\"\n",
    "    Plot median velocities as a function of incubation time using all_metrics_df.\n",
    "\n",
    "    Parameters:\n",
    "    - all_metrics_df (DataFrame): DataFrame containing aggregated metrics for each experiment.\n",
    "    - path_save_pic (str): Path where the plot will be saved.\n",
    "    \"\"\"\n",
    "    # Trier les données par temps d'incubation\n",
    "    all_metrics_sorted = all_metrics_df.sort_values(by='time to incubation (hours)')\n",
    "\n",
    "    # Préparer le tracé\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.scatter(all_metrics_sorted['time to incubation (hours)'], \n",
    "                all_metrics_sorted['mean_speed [um/min]'],  # Correction de la colonne utilisée\n",
    "                color='forestgreen', label='Médiane')\n",
    "    plt.plot(all_metrics_sorted['time to incubation (hours)'], \n",
    "             all_metrics_sorted['mean_speed [um/min]'],  # Correction de la colonne utilisée\n",
    "             linestyle='--', color='limegreen', alpha=0.6)\n",
    "\n",
    "    # Ajouter des labels et des titres\n",
    "    plt.title('Mean Speed vs. Time to Incubation', fontsize=14)\n",
    "    plt.xlabel('Time to Incubation (hours)', fontsize=12)\n",
    "    plt.ylim([0, 15])\n",
    "    plt.ylabel('Mean Speed [μm/min]', fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "    # Sauvegarder et afficher\n",
    "    plt.tight_layout()\n",
    "    fig_path = os.path.join(path_save_pic, 'Mean_Speed_vs_Time.png')\n",
    "    plt.savefig(fig_path, format='png')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Plot saved to {fig_path}\")\n",
    "\n",
    "# Appel de la fonction corrigée\n",
    "plot_median_velocity_vs_time(all_metrics_df, path_save_pic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_proportion_vs_time(all_metrics_df, path_save_pic, img_type):\n",
    "    \"\"\"\n",
    "    Plot the proportion of low coefficient particles (proportion_inf) vs. time to incubation.\n",
    "\n",
    "    Parameters:\n",
    "    - all_metrics_df (DataFrame): DataFrame containing aggregated metrics for each experiment.\n",
    "    - path_save_pic (str): Path where the plot will be saved.\n",
    "    - img_type (str): Image format for saving the plot.\n",
    "    \"\"\"\n",
    "    # Trier les données par temps d'incubation\n",
    "    all_metrics_sorted = all_metrics_df.sort_values(by='time to incubation (hours)')\n",
    "\n",
    "    # Préparer le tracé\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.scatter(all_metrics_sorted['time to incubation (hours)'], \n",
    "                all_metrics_sorted['proportion_inf'], \n",
    "                color='red', marker='+', label='Proportion inf')\n",
    "    \n",
    "    # Ajouter des titres et des labels\n",
    "    plt.title('Proportion of Low Coefficient Particles vs. Time to Incubation', fontsize=14)\n",
    "    plt.xlabel('Time to Incubation (hours)', fontsize=12)\n",
    "    plt.ylabel('Proportion of Low Coefficient Particles', fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "    # Ajuster et sauvegarder\n",
    "    plt.tight_layout()\n",
    "    fig_path = os.path.join(path_save_pic, f'Proportion_Low_Coeff_Particles_vs_Time.{img_type}')\n",
    "    plt.savefig(fig_path, format=img_type)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Plot saved to {fig_path}\")\n",
    "\n",
    "# Appel de la fonction\n",
    "plot_proportion_vs_time(all_metrics_df, path_save_pic, IMG_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_proportion_per_time_bins(all_metrics_df, path_save_pic, img_type):\n",
    "    \"\"\"\n",
    "    Plot proportions of particles (Inf and Sup) per incubation time bins.\n",
    "\n",
    "    Parameters:\n",
    "    - all_metrics_df (DataFrame): DataFrame containing aggregated metrics for each experiment.\n",
    "    - path_save_pic (str): Path where the plot will be saved.\n",
    "    - img_type (str): Image format for saving the plot.\n",
    "    \"\"\"\n",
    "    # Définir les intervalles de temps d'incubation\n",
    "    bins = np.arange(0, all_metrics_df['time to incubation (hours)'].max() + 10, 10)\n",
    "    labels = [f\"{int(left)}-{int(right)}\" for left, right in zip(bins[:-1], bins[1:])]\n",
    "    all_metrics_df['time_bin'] = pd.cut(all_metrics_df['time to incubation (hours)'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "    # Calculer les proportions moyennes pour chaque bin\n",
    "    proportion_per_bin_inf = all_metrics_df.groupby('time_bin')['proportion_inf'].mean()\n",
    "    proportion_per_bin_sup = 1 - proportion_per_bin_inf\n",
    "\n",
    "    # Configuration des barres pour le tracé\n",
    "    x = np.arange(len(proportion_per_bin_inf))\n",
    "    width = 0.35\n",
    "\n",
    "    # Tracé des proportions\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.bar(x - width/2, proportion_per_bin_inf, width, color='red', alpha=0.7, label='Proportion Inf')\n",
    "    ax.bar(x + width/2, proportion_per_bin_sup, width, color='blue', alpha=0.7, label='Proportion Sup')\n",
    "    ax.set_title('Proportion of Particles per Incubation Time Bin', fontsize=14)\n",
    "    ax.set_xlabel('Incubation Time Bin (hours)', fontsize=12)\n",
    "    ax.set_ylabel('Average Proportion of Particles', fontsize=12)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels, rotation=45)\n",
    "    ax.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "    # Sauvegarder et afficher le graphique\n",
    "    plt.tight_layout()\n",
    "    fig_path = os.path.join(path_save_pic, f'Proportion_vs_Time_Bins.{img_type}')\n",
    "    plt.savefig(fig_path, format=img_type)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Plot saved to {fig_path}\")\n",
    "\n",
    "# Appel de la fonction\n",
    "plot_proportion_per_time_bins(all_metrics_df, path_save_pic, IMG_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_proportion_per_time_bins(all_metrics_df, DATA, path_save_pic, img_type):\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Créer les bins\n",
    "    bins = np.arange(0, all_metrics_df['time to incubation (hours)'].max() + 10, 10)\n",
    "    labels = [f\"{int(left)}-{int(right)}\" for left, right in zip(bins[:-1], bins[1:])]\n",
    "    \n",
    "    # Appliquer les bins\n",
    "    all_metrics_df['time_bin'] = pd.cut(all_metrics_df['time to incubation (hours)'], bins=bins, labels=labels, right=False)\n",
    "    DATA['time_bin'] = pd.cut(DATA['time to incubation (hours)'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "    # Moyennes des proportions (métriques par expérience)\n",
    "    proportion_per_bin_inf = all_metrics_df.groupby('time_bin')['proportion_inf'].mean().reindex(labels, fill_value=0)\n",
    "    proportion_per_bin_sup = 1 - proportion_per_bin_inf\n",
    "\n",
    "    # NOMBRE DE PARTICULES UNIQUES PAR BIN (total, low, high)\n",
    "    total, n_low, n_high = compute_particle_counts_per_bin(DATA, bins, labels)\n",
    "\n",
    "    # NOMBRE D'EXPÉRIENCES PAR BIN\n",
    "    count_exp_per_bin = all_metrics_df.groupby('time_bin').size().reindex(labels, fill_value=0)\n",
    "\n",
    "    # PLOT\n",
    "    x = np.arange(len(labels))\n",
    "    width = 0.3\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(15, 7))\n",
    "    bars_inf = ax1.bar(x - width/2, proportion_per_bin_inf, width, color='red', alpha=0.7, label='Proportion Inf (Low)')\n",
    "    bars_sup = ax1.bar(x + width/2, proportion_per_bin_sup, width, color='blue', alpha=0.7, label='Proportion Sup (High)')\n",
    "    ax1.set_ylabel('Average Proportion of Particles', fontsize=12)\n",
    "    ax1.set_xlabel('Incubation Time Bin (hours)', fontsize=12)\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(labels, rotation=45)\n",
    "    ax1.set_ylim(0, 1)\n",
    "    ax1.legend(loc='upper left')\n",
    "\n",
    "    # Second axe : nombre de particules étudiées par bin\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(x, total, 'ko-', label='N Particules (total)')\n",
    "    ax2.plot(x, n_low, 'r.-', label='N Low')\n",
    "    ax2.plot(x, n_high, 'b.-', label='N High')\n",
    "    ax2.set_ylabel('Number of Unique Particles', fontsize=12)\n",
    "    ax2.set_ylim(0, max(total.max(), n_low.max(), n_high.max()) * 1.2)\n",
    "    ax2.legend(loc='upper right')\n",
    "\n",
    "    # Annoter le nombre d’expériences sous chaque bin\n",
    "    for i, count_exp in enumerate(count_exp_per_bin):\n",
    "        ax1.annotate(f\"Exp: {count_exp}\", (x[i], 0), textcoords=\"offset points\", xytext=(0, 10),\n",
    "                     ha='center', fontsize=9, color='k', rotation=90)\n",
    "    # Annoter n_low/n_high sous les barres\n",
    "    for i in range(len(x)):\n",
    "        txt = f\"Low: {int(n_low.iloc[i])}\\nHigh: {int(n_high.iloc[i])}\"\n",
    "        ax1.annotate(txt, (x[i], 0), textcoords=\"offset points\", xytext=(0, -35),\n",
    "                     ha='center', fontsize=9, color='dimgray', rotation=0)\n",
    "\n",
    "    plt.title('Proportion of Particles & Counts per Incubation Time Bin', fontsize=15)\n",
    "    plt.grid(True, linestyle='--', alpha=0.4)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Sauvegarder et afficher\n",
    "    fig_path = os.path.join(path_save_pic, f'Proportion_vs_Time_Bins.{img_type}')\n",
    "    plt.savefig(fig_path, format=img_type)\n",
    "    plt.show()\n",
    "    print(f\"Plot saved to {fig_path}\")\n",
    "\n",
    "def compute_particle_counts_per_bin(DATA, bins, labels):\n",
    "    # Attribuer la time_bin à chaque point\n",
    "    DATA['time_bin'] = pd.cut(DATA['time to incubation (hours)'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "    # Grouper par bin ET particule, puis aggréger\n",
    "    gb = DATA.groupby(['time_bin', 'particle']).agg(\n",
    "        any_inf=('is_inf', 'any'),  # True si la particule a au moins un point \"inf\" dans ce bin\n",
    "        any_sup=('is_sup', 'any')\n",
    "    ).reset_index()\n",
    "\n",
    "    # Nombre total de particules par bin\n",
    "    total = gb.groupby('time_bin')['particle'].nunique().reindex(labels, fill_value=0)\n",
    "    # Nombre \"low\"\n",
    "    n_low = gb.groupby('time_bin')['any_inf'].sum().reindex(labels, fill_value=0)\n",
    "    # Nombre \"high\"\n",
    "    n_high = gb.groupby('time_bin')['any_sup'].sum().reindex(labels, fill_value=0)\n",
    "\n",
    "    return total, n_low, n_high\n",
    "\n",
    "# Appel :\n",
    "plot_proportion_per_time_bins(all_metrics_df, DATA, path_save_pic, IMG_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_proportion_per_time_bins(all_metrics_df, path_save_pic, img_type):\n",
    "    \"\"\"\n",
    "    Plot proportions of particles (Inf and Sup) per incubation time bins, and show number of experiments per bin.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import os\n",
    "\n",
    "    # Définir les intervalles de temps d'incubation\n",
    "    bins = np.arange(0, all_metrics_df['time to incubation (hours)'].max() + 10, 10)\n",
    "    labels = [f\"{int(left)}-{int(right)}\" for left, right in zip(bins[:-1], bins[1:])]\n",
    "    all_metrics_df['time_bin'] = pd.cut(all_metrics_df['time to incubation (hours)'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "    # Calculer les proportions moyennes pour chaque bin\n",
    "    proportion_per_bin_inf = all_metrics_df.groupby('time_bin')['proportion_inf'].mean()\n",
    "    proportion_per_bin_sup = 1 - proportion_per_bin_inf\n",
    "\n",
    "    # Calculer le nombre d’expériences par bin\n",
    "    count_per_bin = all_metrics_df.groupby('time_bin').size()\n",
    "\n",
    "    # Configuration des barres pour le tracé\n",
    "    x = np.arange(len(proportion_per_bin_inf))\n",
    "    width = 0.35\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    # Barres proportions (axe principal)\n",
    "    bars_inf = ax1.bar(x - width/2, proportion_per_bin_inf, width, color='red', alpha=0.7, label='Proportion Inf')\n",
    "    bars_sup = ax1.bar(x + width/2, proportion_per_bin_sup, width, color='blue', alpha=0.7, label='Proportion Sup')\n",
    "    ax1.set_ylabel('Average Proportion of Particles', fontsize=12, color='black')\n",
    "    ax1.set_xlabel('Incubation Time Bin (hours)', fontsize=12)\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(labels, rotation=45)\n",
    "    ax1.set_ylim(0, 1)\n",
    "    ax1.legend(loc='upper left')\n",
    "\n",
    "    # Axe secondaire pour le nombre d’expériences\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(x, count_per_bin, 'ko-', label='N Expériences')\n",
    "    ax2.set_ylabel('Number of Experiments', fontsize=12, color='k')\n",
    "    ax2.set_ylim(0, count_per_bin.max() * 1.2)\n",
    "    ax2.legend(loc='upper right')\n",
    "\n",
    "    # Annoter le nombre d’expériences sous chaque bin\n",
    "    for i, count in enumerate(count_per_bin):\n",
    "        ax2.annotate(str(count), (x[i], count), textcoords=\"offset points\", xytext=(0,5),\n",
    "                     ha='center', fontsize=10, color='black')\n",
    "\n",
    "    plt.title('Proportion of Particles per Incubation Time Bin\\nand Number of Experiments', fontsize=14)\n",
    "    plt.grid(True, linestyle='--', alpha=0.4)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Sauvegarder et afficher le graphique\n",
    "    fig_path = os.path.join(path_save_pic, f'Proportion_vs_Time_Bins.{img_type}')\n",
    "    plt.savefig(fig_path, format=img_type)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Plot saved to {fig_path}\")\n",
    "\n",
    "# Appel de la fonction\n",
    "plot_proportion_per_time_bins(all_metrics_df, path_save_pic, IMG_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot displacement comparison between low and high coefficient particles\n",
    "lib.plot_displacement_low_and_high(\n",
    "    traj_sup=df_sup,\n",
    "    traj_inf=df_inf,\n",
    "    part_coef_inf=PART_COEF_INF,\n",
    "    part_coef_sup=PART_COEF_SUP,\n",
    "    start_end=start_end,\n",
    "    save=True,\n",
    "    pathway_saving=path_save_pic,\n",
    "    name=\"displacement_start-end_time\",\n",
    "    img_type=IMG_TYPE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_displacement_from_start(traj, size_pix):\n",
    "    \"\"\"\n",
    "    Calcule la distance maximale entre le point de départ et toutes les positions\n",
    "    atteintes par chaque particule.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    traj : pandas.DataFrame\n",
    "        Trajectoire des particules avec colonnes 'x', 'y', 'particle'.\n",
    "    size_pix : float\n",
    "        Taille d'un pixel en micromètres.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    max_displacements : pandas.Series\n",
    "        Distance maximale pour chaque particule.\n",
    "    \"\"\"\n",
    "    # Identifier les coordonnées de départ pour chaque particule\n",
    "    start_positions = traj.groupby('particle')[['x', 'y']].first()\n",
    "\n",
    "    # Ajouter les coordonnées de départ au DataFrame\n",
    "    traj = traj.join(start_positions, on='particle', rsuffix='_start')\n",
    "\n",
    "    # Calculer les distances à partir du point de départ\n",
    "    traj['distance_from_start [um]'] = size_pix * np.sqrt(\n",
    "        (traj['x'] - traj['x_start'])**2 + \n",
    "        (traj['y'] - traj['y_start'])**2\n",
    "    )\n",
    "\n",
    "    # Trouver la distance maximale pour chaque particule\n",
    "    max_displacements = traj.groupby('particle')['distance_from_start [um]'].max()\n",
    "\n",
    "    return max_displacements\n",
    "\n",
    "# Calcul des distances maximales\n",
    "max_distances = max_displacement_from_start(DATA, size_pix=SIZE_PIX)\n",
    "\n",
    "# Filtrage des particules appartenant à PART_COEF_INF et PART_COEF_SUP\n",
    "distances_part_coef_inf = max_distances[max_distances.index.isin(PART_COEF_INF)]\n",
    "distances_part_coef_sup = max_distances[max_distances.index.isin(PART_COEF_SUP)]\n",
    "\n",
    "# Tracer les deux histogrammes sur le même graphique\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.hist(distances_part_coef_inf, bins=100, color='red', alpha=0.5, label='PART_COEF_INF')\n",
    "plt.hist(distances_part_coef_sup, bins=100, color='blue', alpha=0.5, label='PART_COEF_SUP')\n",
    "plt.title('Histogramme des distances maximales par catégorie', fontsize=16)\n",
    "plt.xlabel('Distance maximale [μm]', fontsize=14)\n",
    "plt.ylabel('Nombre de particules', fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "X = DATA[['mass', 'size', 'ecc']].dropna()\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(X_scaled)\n",
    "DATA['cluster'] = -1\n",
    "DATA.loc[X.index, 'cluster'] = kmeans.labels_\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(\n",
    "    data=DATA, x='size', y='mass',\n",
    "    hue='cluster', palette=['green', 'orange'],\n",
    "    alpha=0.1, s=0.001, marker='+', linewidth=1.0\n",
    ")\n",
    "plt.title(\"K-means clustering (2 groupes)\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Données préparées comme avant\n",
    "X = DATA[['mass', 'size', 'ecc']].dropna()\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(X_scaled)\n",
    "DATA['cluster'] = -1\n",
    "DATA.loc[X.index, 'cluster'] = kmeans.labels_\n",
    "\n",
    "# Création des deux sous-graphes côte à côte\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6), sharex=True, sharey=True)\n",
    "\n",
    "for i in range(2):\n",
    "    sns.scatterplot(\n",
    "        data=DATA[DATA['cluster'] == i],\n",
    "        x='size', y='mass',\n",
    "        ax=axes[i],\n",
    "        color='green' if i == 0 else 'orange',\n",
    "        alpha=1, s=10, marker='+', linewidth=1\n",
    "    )\n",
    "    axes[i].set_title(f\"Groupe {i}\")\n",
    "    axes[i].grid(True)\n",
    "\n",
    "fig.suptitle(\"Visualisation des 2 groupes K-means\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Séparation des clusters en deux DataFrames\n",
    "DATA_cluster0 = DATA[DATA['cluster'] == 0].copy()\n",
    "DATA_cluster1 = DATA[DATA['cluster'] == 1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recompute trajectory clustering with new cutoff\n",
    "# DATA_intermediaire = DATA.copy()\n",
    "DATA_cluster0['frame'] = pd.factorize(DATA_cluster0['frame'])[0]\n",
    "IMSD_cluster0 = tp.imsd(traj=DATA_cluster0, mpp=SIZE_PIX, fps=FPS, max_lagtime=200, statistic='msd')\n",
    "LAG_TIME_FIT = 5\n",
    "COEF_INF_cluster0, COEF_SUP_cluster0, PART_COEF_INF_cluster0, PART_COEF_SUP_cluster0, CUTOFF_cluster0 = lib.traj_clustering_with_fit_cutoff(\n",
    "    DATA_cluster0, imsd=IMSD_cluster0, hist=True, lag_time_fit_start=0, lag_time_fit=LAG_TIME_FIT , micronperpixel=SIZE_PIX,\n",
    "    fps=FPS, binsize=250, peak_height=50, peak_width=1, save=True, pathway_fig=path_save_pic,\n",
    "    name='MSD and slopes cluster1', img_type=IMG_TYPE, plot=True, color_sup_inf=color_sup_inf,\n",
    "    cutoff_default=0.6\n",
    ")\n",
    "\n",
    "# Ajouter les colonnes 'is_inf' et 'is_sup' à DATA\n",
    "DATA_cluster0['particle'] = DATA_cluster0['particle'].astype(int)\n",
    "PART_COEF_INF_cluster0 = set(map(int, PART_COEF_INF_cluster0))\n",
    "PART_COEF_SUP_cluster0 = set(map(int, PART_COEF_SUP_cluster0))\n",
    "\n",
    "# Ajout des colonnes 'is_inf' et 'is_sup'\n",
    "DATA_cluster0['is_inf'] = DATA_cluster0['particle'].isin(PART_COEF_INF_cluster0).astype(int)\n",
    "DATA_cluster0['is_sup'] = DATA_cluster0['particle'].isin(PART_COEF_SUP_cluster0).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recompute trajectory clustering with new cutoff\n",
    "# DATA_intermediaire = DATA.copy()\n",
    "DATA_cluster1['frame'] = pd.factorize(DATA_cluster1['frame'])[0]\n",
    "IMSD_cluster1 = tp.imsd(traj=DATA_cluster1, mpp=SIZE_PIX, fps=FPS, max_lagtime=200, statistic='msd')\n",
    "LAG_TIME_FIT = 5\n",
    "COEF_INF_cluster1, COEF_SUP_cluster1, PART_COEF_INF_cluster1, PART_COEF_SUP_cluster1, CUTOFF_cluster1 = lib.traj_clustering_with_fit_cutoff(\n",
    "    DATA_cluster1, imsd=IMSD_cluster1, hist=True, lag_time_fit_start=0, lag_time_fit=LAG_TIME_FIT , micronperpixel=SIZE_PIX,\n",
    "    fps=FPS, binsize=250, peak_height=50, peak_width=1, save=True, pathway_fig=path_save_pic,\n",
    "    name='MSD and slopes cluster2', img_type=IMG_TYPE, plot=True, color_sup_inf=color_sup_inf,\n",
    "    cutoff_default=0.6\n",
    ")\n",
    "# Ajouter les colonnes 'is_inf' et 'is_sup' à DATA\n",
    "DATA_cluster1['particle'] = DATA_cluster1['particle'].astype(int)\n",
    "PART_COEF_INF_cluster1 = set(map(int, PART_COEF_INF_cluster1))\n",
    "PART_COEF_SUP_cluster1 = set(map(int, PART_COEF_SUP_cluster1))\n",
    "# Ajout des colonnes 'is_inf' et 'is_sup'\n",
    "DATA_cluster1['is_inf'] = DATA_cluster1['particle'].isin(PART_COEF_INF_cluster1).astype(int)\n",
    "DATA_cluster1['is_sup'] = DATA_cluster1['particle'].isin(PART_COEF_SUP_cluster1).astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for experiment in data_unique_pos['experiment'].unique():\n",
    "#     # Sous-ensemble pour cette expérience\n",
    "#     sub_df = data_unique_pos[data_unique_pos['experiment'] == experiment].copy()\n",
    "\n",
    "#     # Filtrage spatial : éliminer particules trop proches\n",
    "#     coords = sub_df[['x', 'y']].to_numpy()\n",
    "#     if len(coords) > 1:\n",
    "#         nbrs = NearestNeighbors(n_neighbors=2).fit(coords)\n",
    "#         distances, _ = nbrs.kneighbors(coords)\n",
    "#         keep_mask = distances[:, 1] > min_distance\n",
    "#         sub_df = sub_df[keep_mask].copy()\n",
    "\n",
    "#     # Recherche du dossier correspondant à l'expérience\n",
    "#     experiment_dirs = glob.glob(os.path.join(base_image_dir, f\"*{experiment}*\"))\n",
    "#     if not experiment_dirs:\n",
    "#         print(f\"Dossier pour {experiment} non trouvé !\")\n",
    "#         continue\n",
    "#     experiment_dir = experiment_dirs[0]\n",
    "\n",
    "#     # Chargement de l'image frame 0\n",
    "#     img_path = os.path.join(experiment_dir, \"mosaic/mosaic_total_0.tif\")\n",
    "#     if not os.path.isfile(img_path):\n",
    "#         print(f\"Image {img_path} non trouvée !\")\n",
    "#         continue\n",
    "\n",
    "#     img = cv2.imread(img_path)\n",
    "#     if img is None:\n",
    "#         print(f\"Image {img_path} illisible !\")\n",
    "#         continue\n",
    "\n",
    "#     # Tracer les cercles\n",
    "#     for _, row in sub_df.iterrows():\n",
    "#         x = int(round(row[\"x\"]))\n",
    "#         y = int(round(row[\"y\"]))\n",
    "#         cv2.circle(img, (x, y), 50, (0, 0, 255), 3)\n",
    "\n",
    "#     # Sauvegarde\n",
    "#     out_path = os.path.join(output_dir, f\"{experiment}_mosaic_total_0_inf_filtered.png\")\n",
    "#     cv2.imwrite(out_path, img)\n",
    "#     print(f\"Image annotée sauvegardée : {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # === PARAMÈTRES ===\n",
    "# base_image_dir = \"/Users/souchaud/Desktop/A_analyser/CytoOne_HL5_10x_faits/\"\n",
    "# output_dir = \"/Users/souchaud/Desktop/faibles/\"\n",
    "# min_distance = 10  # en pixels\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# # === 1. Extraire particules du cluster 1, faibles ===\n",
    "# data_inf = DATA[(DATA['cluster'] == 1) & (DATA['is_inf'] == 1)].copy()\n",
    "\n",
    "# # === 2. Trouver la première frame de chaque particule ===\n",
    "# first_frame_df = data_inf.sort_values('frame').groupby('particle', as_index=False).first()\n",
    "# first_frame_df['start_frame'] = first_frame_df['frame']  # juste pour clarté\n",
    "\n",
    "# # === 3. Filtrage spatial dans chaque frame de début ===\n",
    "# filtered_particles = []\n",
    "\n",
    "# for f in first_frame_df['start_frame'].unique():\n",
    "#     same_frame = first_frame_df[first_frame_df['start_frame'] == f].copy()\n",
    "    \n",
    "#     if len(same_frame) <= 1:\n",
    "#         filtered_particles.append(same_frame)\n",
    "#         continue\n",
    "\n",
    "#     coords = same_frame[['x', 'y']].to_numpy()\n",
    "#     nbrs = NearestNeighbors(n_neighbors=2).fit(coords)\n",
    "#     distances, _ = nbrs.kneighbors(coords)\n",
    "#     keep_mask = distances[:, 1] > min_distance\n",
    "#     filtered_particles.append(same_frame[keep_mask])\n",
    "\n",
    "# # === 4. Particules uniques à annoter ===\n",
    "# final_df = pd.concat(filtered_particles, ignore_index=True)\n",
    "\n",
    "# # === 5. Génération des images par expérience ===\n",
    "# for experiment in final_df['experiment'].unique():\n",
    "#     sub_df = final_df[final_df['experiment'] == experiment].copy()\n",
    "\n",
    "#     # Localise le dossier de l’expérience\n",
    "#     experiment_dirs = glob.glob(os.path.join(base_image_dir, f\"*{experiment}*\"))\n",
    "#     if not experiment_dirs:\n",
    "#         print(f\"Dossier pour {experiment} non trouvé !\")\n",
    "#         continue\n",
    "#     experiment_dir = experiment_dirs[0]\n",
    "\n",
    "#     # Charge l'image frame 0\n",
    "#     img_path = os.path.join(experiment_dir, \"mosaic/mosaic_total_0.tif\")\n",
    "#     if not os.path.isfile(img_path):\n",
    "#         print(f\"Image {img_path} non trouvée !\")\n",
    "#         continue\n",
    "\n",
    "#     img = cv2.imread(img_path)\n",
    "#     if img is None:\n",
    "#         print(f\"Image {img_path} illisible !\")\n",
    "#         continue\n",
    "\n",
    "#     # Trace les cercles\n",
    "#     for _, row in sub_df.iterrows():\n",
    "#         x = int(round(row[\"x\"]))\n",
    "#         y = int(round(row[\"y\"]))\n",
    "#         cv2.circle(img, (x, y), 50, (0, 0, 255), 3)\n",
    "\n",
    "#     # Sauvegarde\n",
    "#     out_path = os.path.join(output_dir, f\"{experiment}_mosaic_total_0_inf_filtered.png\")\n",
    "#     cv2.imwrite(out_path, img)\n",
    "#     print(f\"Image annotée sauvegardée : {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On prend le cluster majoritaire par particule\n",
    "df_particles = DATA.groupby('particle')['cluster'].agg(lambda x: x.value_counts().index[0]).reset_index()\n",
    "\n",
    "# On compte le nombre de particules annotées 0 en cluster\n",
    "n_particles_cluster0 = (df_particles['cluster'] == 1).sum()\n",
    "\n",
    "print(f\"Nombre de particules annotées cluster 0 : {n_particles_cluster0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour chaque particule, associer le cluster majoritaire, et la classe annotée (is_sup, is_inf)\n",
    "df_comp = DATA.groupby('particle').agg({\n",
    "    'cluster': lambda x: x.value_counts().index[0],  # cluster le plus fréquent\n",
    "    'is_sup': 'first',    # ou 'any' si tu as de rares cellules mixtes\n",
    "    'is_inf': 'first'\n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple : correspondance cluster <-> is_sup\n",
    "# (on suppose cluster 1 doit matcher is_sup=True, à vérifier selon ton contexte)\n",
    "correspondance_sup = df_comp[df_comp['cluster'] == 0]['is_sup'].mean()\n",
    "correspondance_inf = df_comp[df_comp['cluster'] == 1]['is_inf'].mean()\n",
    "\n",
    "print(f\"Taux de cellules du cluster 1 annotées SUP : {correspondance_sup:.2%}\")\n",
    "print(f\"Taux de cellules du cluster 0 annotées INF : {correspondance_inf:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(DATA['size'], DATA['mass'], DATA['ecc'], c=DATA['cluster'], cmap='Set1', alpha=0.4, marker='.', s=1)\n",
    "ax.set_xlabel('size')\n",
    "ax.set_ylabel('mass')\n",
    "ax.set_zlabel('ecc')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_speed_vs_density(all_metrics_df, save=False, pathway_saving=None, img_type=\"png\"):\n",
    "    \"\"\"\n",
    "    Trace la vitesse moyenne des particules en fonction de la densité cellulaire,\n",
    "    en utilisant uniquement Matplotlib et des croix pour les points.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    all_metrics_df : pandas.DataFrame\n",
    "        Contient les colonnes 'mean_speed [um/min]' et 'nombre_part_par_champs'.\n",
    "    save : bool, optional\n",
    "        Si True, sauvegarde le graphique.\n",
    "    pathway_saving : str, optional\n",
    "        Chemin pour sauvegarder le graphique si save=True.\n",
    "    img_type : str, optional\n",
    "        Format d'image pour la sauvegarde (png, jpg, etc.).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Vérification des colonnes nécessaires\n",
    "    if not all(col in all_metrics_df.columns for col in ['mean_speed [um/min]', 'nombre_part_par_champs']):\n",
    "        raise ValueError(\"Les colonnes 'mean_speed [um/min]' et 'nombre_part_par_champs' doivent être présentes dans all_metrics_df.\")\n",
    "    \n",
    "    # Créer le graphique\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    ax = plt.gca()  # Récupérer les axes actuels avec le style Matplotlib\n",
    "\n",
    "    # Tracé des points avec des croix\n",
    "    ax.scatter(\n",
    "        all_metrics_df['nombre_part_par_champs'],\n",
    "        all_metrics_df['mean_speed [um/min]'],\n",
    "        s=100,  # Taille des symboles\n",
    "        alpha=0.7,\n",
    "        color='#1f77b4',  # Couleur compatible avec axes.prop_cycle\n",
    "        marker='+',  # Utiliser des croix\n",
    "        label=\"Points\"\n",
    "    )\n",
    "\n",
    "    # Régression linéaire\n",
    "    x = all_metrics_df['nombre_part_par_champs']\n",
    "    y = all_metrics_df['mean_speed [um/min]']\n",
    "    coeffs = np.polyfit(x, y, 1)  # Ajustement linéaire\n",
    "    y_fit = np.polyval(coeffs, x)\n",
    "    ax.plot(x, y_fit, color='#ff7f0e', label=\"Régression linéaire\")  # Ligne de tendance\n",
    "\n",
    "    # Titre et axes\n",
    "    ax.set_title(\"Vitesse moyenne vs Densité cellulaire\", fontsize=16, color='white')\n",
    "    ax.set_xlabel(\"Nombre de particules par champ\", fontsize=14, color='white')\n",
    "    ax.set_ylabel(\"Vitesse moyenne [µm/min]\", fontsize=14, color='white')\n",
    "    ax.legend(fontsize=12)\n",
    "    ax.grid(alpha=0.4)\n",
    "\n",
    "    # Sauvegarder ou afficher\n",
    "    if save and pathway_saving:\n",
    "        filename = f\"{pathway_saving}/mean_speed_vs_density.{img_type}\"\n",
    "        plt.savefig(filename, format=img_type, bbox_inches=\"tight\", facecolor=plt.rcParams[\"figure.facecolor\"])\n",
    "        print(f\"Graphique sauvegardé : {filename}\")\n",
    "    plt.show()\n",
    "\n",
    "# Exemple d'utilisation\n",
    "plot_speed_vs_density(\n",
    "    all_metrics_df=all_metrics_df,\n",
    "    save=False,\n",
    "    pathway_saving=None,\n",
    "    img_type=\"png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_speed_vs_density(all_metrics_df, save=False, pathway_saving=None, img_type=\"png\"):\n",
    "    \"\"\"\n",
    "    Trace la vitesse moyenne des particules en fonction de la densité cellulaire,\n",
    "    en respectant les paramètres Matplotlib globaux.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    all_metrics_df : pandas.DataFrame\n",
    "        Contient les colonnes 'mean_speed [um/min]' et 'nombre_part_par_champs'.\n",
    "    save : bool, optional\n",
    "        Si True, sauvegarde le graphique.\n",
    "    pathway_saving : str, optional\n",
    "        Chemin pour sauvegarder le graphique si save=True.\n",
    "    img_type : str, optional\n",
    "        Format d'image pour la sauvegarde (png, jpg, etc.).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Vérification des colonnes nécessaires\n",
    "    if not all(col in all_metrics_df.columns for col in ['mean_speed [um/min]', 'nombre_part_par_champs']):\n",
    "        raise ValueError(\"Les colonnes 'mean_speed [um/min]' et 'nombre_part_par_champs' doivent être présentes dans all_metrics_df.\")\n",
    "    \n",
    "    # Désactiver le style Seaborn pour respecter Matplotlib\n",
    "    sns.set_theme(style=None)\n",
    "\n",
    "    # Créer le graphique\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    ax = plt.gca()  # Récupérer les axes actuels avec le style Matplotlib\n",
    "\n",
    "    # Tracé des points avec scatterplot\n",
    "    sns.scatterplot(\n",
    "        x='nombre_part_par_champs',\n",
    "        y='mean_speed [um/min]',\n",
    "        data=all_metrics_df,\n",
    "        s=100,  # Taille des points\n",
    "        alpha=0.7,\n",
    "        color='#1f77b4',  # Couleur compatible avec axes.prop_cycle\n",
    "        edgecolor='white'\n",
    "    )\n",
    "\n",
    "    # Ajouter une ligne de tendance avec régression\n",
    "    sns.regplot(\n",
    "        x='nombre_part_par_champs',\n",
    "        y='mean_speed [um/min]',\n",
    "        data=all_metrics_df,\n",
    "        scatter=False,\n",
    "        color='#ff7f0e',  # Deuxième couleur du cycle\n",
    "        line_kws={'label': \"Régression linéaire\"}\n",
    "    )\n",
    "\n",
    "    # Titre et axes\n",
    "    ax.set_title(\"Vitesse moyenne vs Densité cellulaire\", fontsize=16, color='white')\n",
    "    ax.set_xlabel(\"Nombre de particules par champ\", fontsize=14, color='white')\n",
    "    ax.set_ylabel(\"Vitesse moyenne [µm/min]\", fontsize=14, color='white')\n",
    "    ax.legend(fontsize=12)\n",
    "    ax.grid(alpha=0.4)\n",
    "\n",
    "    # Sauvegarder ou afficher\n",
    "    if save and pathway_saving:\n",
    "        filename = f\"{pathway_saving}/mean_speed_vs_density.{img_type}\"\n",
    "        plt.savefig(filename, format=img_type, bbox_inches=\"tight\", facecolor=plt.rcParams[\"figure.facecolor\"])\n",
    "        print(f\"Graphique sauvegardé : {filename}\")\n",
    "    plt.show()\n",
    "\n",
    "# utilisation\n",
    "plot_speed_vs_density(\n",
    "    all_metrics_df=all_metrics_df,\n",
    "    save=False,\n",
    "    pathway_saving=None,\n",
    "    img_type=\"png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_condition_and_save_hdf5(data, condition, name, save_path):\n",
    "    \"\"\"\n",
    "    Ajoute une colonne 'condition' à DATA et sauvegarde le DataFrame au format HDF5.\n",
    "\n",
    "    Parameters:\n",
    "    - DATA (pd.DataFrame): DataFrame auquel ajouter la colonne.\n",
    "    - CONDITION_simple (str): Valeur à ajouter dans la colonne 'condition'.\n",
    "    - save_path (str): Chemin pour sauvegarder le fichier HDF5.\n",
    "    -name (str): détail du fichier enregistré\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Ajouter la colonne 'condition'\n",
    "    data['condition'] = condition\n",
    "\n",
    "    # Vérifier si le DataFrame est non vide\n",
    "    if data.empty:\n",
    "        raise ValueError(\"Le DataFrame est vide, aucune donnée à sauvegarder.\")\n",
    "\n",
    "    # Sauvegarder au format HDF5\n",
    "    data.to_hdf(save_path + condition + f\"_{name}.hdf5\", key='DATA', mode='w', format='table')\n",
    "    print(f\"DataFrame {name} sauvegardé avec la colonne 'condition' à l'emplacement : {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_condition_and_save_hdf5(data=DATA, condition=CONDITION_simple, name=\"data\", save_path=\"/Users/souchaud/Desktop/Analyses/tables_resultats/\")\n",
    "add_condition_and_save_hdf5(data=all_metrics_df, condition=CONDITION_simple, name=\"all_metrics\", save_path=\"/Users/souchaud/Desktop/Analyses/tables_resultats/\")\n",
    "add_condition_and_save_hdf5(data=metrics_df, condition=CONDITION_simple, name=\"metrics\", save_path=\"/Users/souchaud/Desktop/Analyses/tables_resultats/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(PART_COEF_INF)/(len(PART_COEF_INF)+len(PART_COEF_SUP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(PART_COEF_SUP)/(len(PART_COEF_INF)+len(PART_COEF_SUP))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tracking_and_analyse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
